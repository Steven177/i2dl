{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classifier / Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having worked with the Dataloading part last week, we want to start this week to take a more detailed look into how the training process looks like. So far, our tools are limited and we must restrict ourselves to a simplified model. But nevertheless, this gives us the opportunity to look at the different parts of the training process in more detail and builds up a good base when we turn to more complicated model architectures in the next exercises. \n",
    "\n",
    "This notebook will demonstrate a simple logistic regression model predicting whether a house is ```low-priced``` or ```expensive```. The data that we will use here is the HousingPrice dataset. Feeding some features in our classifier, the output should then be a score that determines in which category the considered house is.\n",
    "\n",
    "![classifierTeaser](images/classifierTeaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us first import some libraries and code that we will need along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Dataloading and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start preprocessing our data, let us first download the dataset and use the ```CSVDataset``` class to access the downloaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "target_column = 'SalePrice'\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the dataset in ```i2dl_exercises/datasets/housing``` in your file browser, which should contain a csv file containing all the data. \n",
    "\n",
    "It is always a good idea to get an overview of how our dataset looks like. By executing the following cell you can see some data samples. For each house, our dataset provides 81 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 80 features of our models. But not all the features are correlated with our target 'SalePrice'. So we need to perform a feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df.corr()[target_column].sort_values(ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our classifier is a very simple version we restrict our model to only one of the given features. In our case, let us select the feature ```GrLivArea``` and use this one to predict the target column , which will be the feature ```SalePrice```. This setting has the advantage that we can easiliy visualize our data in a 2 dimensional setting. Of course, a greater choice of features would make our model more powerful and accurate. But as we said, we want to keep it simple here and focus on the training process. The required data for training our model will then reduce to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected feature and target \n",
    "train_dataset.df[['GrLivArea',target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a scatter plot, we can visualize the relationship between ‘GrLivArea’ and 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_dataset.df[['GrLivArea']], train_dataset.df[[target_column]])\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are at very different scales and variances. Therfore, we normalize the features ranges with the minimum and maximum value of each numeric column. For filling in missing numeric values (if any), we need the mean value. These values should be pre-computed on the training set and used for all dataset splits. \n",
    "\n",
    "The ```FeatureSelectorAndNormalizationTransform``` class defined in ```exercise_code/data/csv_dataset.py``` is implementing this transformation. Make sure you have a look at the code of this file to understand the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having computed the ```min```, ```max``` and ```mean``` value, we load the data splits and perform the transformation on our data using the ```CSVDataset``` class. To check whether the partitions are correct, we print for each one of them the number of samples. Remember to not touch the test set until you are done with the training of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the respective data splits ('train', 'val, and 'test') into one matrix of shape $N \\times D$ where $N$ represents the number of samples and $D$ the number of features (in our case we only have one feature). Similarly, we load the target data in one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into a matrix of shape (N, D), same for targets resulting in the shape (N, 1)\n",
    "X_train = [train_dataset[i]['features'] for i in range((len(train_dataset)))]\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = [train_dataset[i]['target'] for i in range((len(train_dataset)))]\n",
    "y_train = np.stack(y_train, axis=0)\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "\n",
    "# load validation data\n",
    "X_val = [val_dataset[i]['features'] for i in range((len(val_dataset)))]\n",
    "X_val = np.stack(X_val, axis=0)\n",
    "y_val = [val_dataset[i]['target'] for i in range((len(val_dataset)))]\n",
    "y_val = np.stack(y_val, axis=0)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "\n",
    "# load train data\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# 0 encodes small prices, 1 encodes large prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we model our binary classification problem. We divide our target in the categories ```low-priced``` and ```expensive``` by labeling the 30% of the houses that are sold with the lowest price with ```0``` and, accordingly, the 30% of the houses with the highest price with ```1```. All other houses will be deleted from our data. We will use the  method ```binarize()```. For more information, take a look at the file ```networks/utils.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.utils import binarize\n",
    "y_all = np.concatenate([y_train, y_val, y_test])\n",
    "thirty_percentile = np.percentile(y_all, 30)\n",
    "seventy_percentile = np.percentile(y_all, 70)\n",
    "\n",
    "# Prepare the labels for classification.\n",
    "X_train, y_train = binarize(X_train, y_train, thirty_percentile, seventy_percentile )\n",
    "X_val, y_val   = binarize(X_val, y_val, thirty_percentile, seventy_percentile)\n",
    "X_test, y_test  = binarize(X_test, y_test, thirty_percentile, seventy_percentile)\n",
    "\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we reduced our data and the remaining houses in our dataset are now either labeled with ```1``` and hence categorized as ```expensive```, or they are labeled with ```0``` and hence categorized as ```low-priced```.\n",
    "\n",
    "The data is now ready and can be used to train our classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a classfier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ be our data with $N$ samples and $D$ feature dimensions. With our classifier model, we want to predict binary labels $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{N\\times 1}$. Our classifier model should be of the form\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}  = \\sigma \\left( \\mathbf{X} \\cdot \\mathbf{w} \\right), $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight matrix of our model.\n",
    "\n",
    "The **sigmoid function** $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n",
    "\n",
    "$$ \\sigma(t) = \\frac{1}{1+e^{-t}}, $$\n",
    "\n",
    "is used to squash the outputs of the linear layer into the interval $[0, 1]$. Remember that the sigmoid function is a real-valued function. When applying it on a vector, the sigmoid is operating componentwise.\n",
    "\n",
    "The output of the sigmoid function can be seen as the probability that our sample is indicating a house that can be categorized as ```expensive```. As the probability gets closer to 1, our model is more confident that the input sample is in the class ```expensive```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the implementation of the ```Classifier``` class in `exercise_code/networks/classifier.py`. To create a `Classifier` object, you need to define the number of features that our classifier models takes as input. \n",
    "\n",
    "Furthermore, the class provides a method `initialize_weights()` that can be used to randomly initialize the weights of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss: Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification like our task, we use a loss function called Binary Cross Entropy (BCE).\n",
    "\n",
    "$$BCE(y,\\hat{y}) =- y \\cdot log(\\hat y ) - (1- y) \\cdot log(1-\\hat y) $$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the ground truth and $\\hat y\\in\\mathbb{R}$ is the predicted probability of the house being expensive.\n",
    "\n",
    "BCE can be understood as two separate cost functions: One for ground truth $y=0$ and one for $y=1$. \n",
    "\n",
    "$$BCE(y=0,\\hat{y}) = - log(1-\\hat y)$$ \n",
    "$$BCE(y=1,\\hat{y}) = - log(\\hat y )$$ \n",
    "\n",
    "Since the BCE functioon is a non-convex function, there is no closed-form solution for the optimal weights vector. In order to find the optimal parameters for our model, we need to use numeric methods such as Gradient Descent. But let us have a look at that later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for your first task: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement the BCE loss function\n",
    "In `exercise_code/networks/loss.py` complete the implementation of the BCE loss function. You need to write the forward and backward pass of BCE as `forward()` and `backward()` function. The backward pass of the loss is needed to later optimize your weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Check\n",
    "\n",
    "Once you have finished the implementation of the BCE loss class, you can run the following code to check whether your forward result and backward gradient are correct. You should expect your relative error to be lower than 1e-8.\n",
    "\n",
    "Here we will use a numeric gradient check to debug the backward pass:\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "where $h$ is a very small number, in practice approximately 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "print (BCETest(bce_loss)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm allows the information from the loss flowing backward through the network in order to compute the gradient of the loss function $L$ w.r.t the weights $w$ of the model. \n",
    "\n",
    "The key idea of backpropagation is decomposing the derivatives by applying the chain rule to the loss function.\n",
    "\n",
    "$$ \\frac{\\partial L(w)}{\\partial w} = \\frac{\\partial L(w)}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial w}$$\n",
    "\n",
    "It means that we need to compute the gradient of the loss function $L$ w.r.t to predictions $\\hat y$ and the gradient of predictions $\\hat y$ w.r.t weights $w$ separately. \n",
    "\n",
    "You have already completed the `forward()` and `backward()` pass of the loss function, which can be used to compute the derivative  $\\frac{\\partial L(w)}{\\partial \\hat y}$. In order to compute the second term $\\frac{\\partial \\hat y}{\\partial w}$, we need to implement a similar `forward()` and `backward()` method in our `Classifier` class.\n",
    "\n",
    "**Forward-pass**\n",
    "\n",
    "Our classifier is given by\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}  = \\sigma \\left( \\mathbf{X} \\cdot  w \\right) $$\n",
    "\n",
    "Hence the forward pass consists of two parts: The multiplication of our data matrix $X \\in \\mathbb{R}^{N \\times D+1}$ with the weight matrix of our model $w \\in \\mathbb{R}^{D+1}$ and the sigmoid function $\\sigma: \\mathbb{R} \\rightarrow [0,1]$ that is operating componentwise on the vector $ X \\cdot w$.\n",
    "\n",
    "**Backward-propagation**\n",
    "\n",
    "The backward-pass is definitely the more complicated part here and consists of computing $\\frac{\\partial \\hat y}{\\partial w}$. Again, we can decompose this derivative into two parts: Let $s = X \\cdot w$ and hence we can decompose the term:\n",
    "\n",
    "$$\\frac{\\partial \\hat y}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial s} \\cdot \\frac{\\partial s}{\\partial w}$$\n",
    "\n",
    "\n",
    "**Hint:** From the mathematical point of view, the backward pass is not very difficult, but taking track of the dimensions in higher dimensional settings can make it complicated. Make sure you understand the operations here. If you find it difficult, maybe it helps to understand the forward and backward pass if the input is only one sample consisting of $D+1$ features. Then our data matrix has dimension $X \\in \\mathbb{R}^{1 \\times D+1}$. After you understood this situation, you can go back to the setting where our data matrix has dimension $X \\in \\mathbb{R}^{N \\times D+1}$ and consists of $N$ samples each having $D+1$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement the forward- and backward-pass in the Classifier model\n",
    "Implement the `forward()` and `backward()` pass in the `Classifier` class in `exercise_code/networks/classifier.py`. Don't forget to also implement the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TESTING TEAM: Insert a test to check whether the implementation of the forward() and backward() methods are correct.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have successfully dealt with the loss function, which is a method of measuring how well our model fits the given data. The idea of the training process is to adjust iteratively the weights of our model in order to minimize the loss function. \n",
    "\n",
    "And this is where the optimizer comes in. In each training step, the optimizer updates the weights of the model w.r.t. the output of the loss function, thereby linking the loss function and model parameters together. The goal is to obtain a model which is accurately predicting the class for a sample.\n",
    "\n",
    "In other words, the loss function is a guide to the terrain and can tell the optimizer when to move in the right or wrong direction.\n",
    "\n",
    "Any discussion about optimizers needs to begin with the most popular one, and it’s called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It’s fast, robust, and flexible. Here’s how it works:\n",
    "\n",
    "\n",
    "0. Initialize the weights with random values. \n",
    "1. Calculate loss with the current weights and the loss function.\n",
    "2. Calculate the gradient of the loss function w.r.t. the weights\n",
    "3. Update weights with the corresponding gradient\n",
    "4. Iteratively perform Step 1 to 3 until converges\n",
    "\n",
    "The name of the optimizer already hints to the required concept: We use gradients which are very useful for minimizing a function. The gradient of the loss function w.r.t to the weights $w$ of our model tells us how to change our weights $w$ in order to minimize our loss function. \n",
    "\n",
    "The weights are updated each step as follows:\n",
    "$$ w^{(n+1)} = w^{(n)} - \\alpha \\cdot \\frac {dL}{dw}, $$\n",
    "$ $ where $ \\frac {dL}{dw}$ is the gradient of your loss function w.r.t. the weights $w$, $\\alpha$ is the learning rate which is a predefined positive scalar determining the size of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 3: Implement a Naive Optimizer using Gradient Descent \n",
    "\n",
    "In our model, we will use gradient descent to update the weights. Take a look at the `Optimizer` class in the file `networks/optimizer.py`. Your task is now to implement the gradient descent step in the `step()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TESTING TEAM: Insert a test to check whether the implementation of step() method is correct. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now implemented all necessary parts of our training process, namely:\n",
    "- **Classifier Model:** We set up a simple classifier model and you implemented the corresponding ```forward()``` and ```backward()``` methods.\n",
    "- **Loss function:** We chose the Binary Cross Entropy Loss for our model to measure the distance between the prediction of our model and the ground-truth labels. You implemented a forward and backward pass for the loss function.\n",
    "- **Optimizer**:We use the Gradient Descent method to update the weights of our model. Here, you implemented the ```step()``` function which performs the update of the weights. \n",
    "\n",
    "Before we start our training and put all the parts together, let us shortly talk about the weight initialization. In ```networks/classifier.py``` you can check the ```Classifier``` class. It contains a method called ```initialize_weights()``` that randomly initializes the weights of our classifier model. Later in the lecture, we will learn about more efficient methods to initialize the weights. But for now, a random initialization as it happens in the ```initialize_weights()``` method is sufficient.  \n",
    "\n",
    "Let's start with our classifier model and look at it's performance before any training happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "#initialization\n",
    "model = Classifier(num_features=1)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the predictions of our model are really bad when we randomly initialize the weights (which is of course not surprising). Let's see how the performance improves when we start our training, which means that we update our weights applying the gradient descent method. The following cell combines the forward- and backward passes with the gradient update step and performs a training step for our classifier:\n",
    "\n",
    "Note that the ```Classifier``` class is derived from the more general ```Network``` class. It is worth having a look at the basis class ```Network``` in the file ```exercise_code/networks/base_networks.py```. We will make use of the ```__call__()``` method, which computes the forward and backward pass of your classifier. In a similar manner, we use the ```__call__()``` function for our Loss function.\n",
    "\n",
    "The following cell performs a training with 800 training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import *\n",
    "from exercise_code.networks.classifier import *\n",
    "# Hyperparameter Setting, we will specify the loss function we use, and implement the optimizer we finished in the last step.\n",
    "num_features = 1\n",
    "\n",
    "# initialization\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "loss_func = BCE() \n",
    "learning_rate = 5e-1\n",
    "loss_history = []\n",
    "opt = Optimizer(model,learning_rate)\n",
    "\n",
    "steps = 800\n",
    "# Full batch Gradient Descent\n",
    "for i in range(steps):\n",
    "    \n",
    "    # Enable your model to store the gradient.\n",
    "    model.train()\n",
    "    \n",
    "    # Compute the output and gradients w.r.t weights of your model for the input dataset.\n",
    "    model_forward, model_backward = model(X_train)\n",
    "    \n",
    "    # Compute the loss and gradients w.r.t output of the model.\n",
    "    loss, loss_grad = loss_func(model_forward, y_train)\n",
    "    \n",
    "    # Use back prop method to get the gradients of loss w.r.t the weights.\n",
    "    grad = loss_grad * model_backward\n",
    "    \n",
    "    # Compute the average gradient over your batch\n",
    "    grad = np.mean(grad, 0, keepdims = True)\n",
    "\n",
    "    # After obtaining the gradients of loss with respect to the weights, we can use optimizer to\n",
    "    # do gradient descent step.\n",
    "    opt.step(grad.T)\n",
    "    \n",
    "    # Average over the loss of the entire dataset and store it.\n",
    "    average_loss = np.mean(loss)\n",
    "    loss_history.append(average_loss)\n",
    "    if i%100 == 0:\n",
    "        print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our average loss is decreasing as we expected it. Let us visualize the average loss and the prediction after our short training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history to see how it goes after several steps of gradient descent.\n",
    "plt.plot(loss_history, label = 'Train Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train, label = 'Ground Truth')\n",
    "inds = X_train.argsort(0).flatten()\n",
    "plt.plot(X_train[inds], y_out[inds], color='r', label = 'Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good already and our model gets better in explaining the underlying relationship of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put everything we have learned so far together in an organized and concise way, that provides easy access to train a network/model in your own script/code. The purpose of a solver is to mainly to provide an abstraction for all the gritty details behind training your parameters, such as logging your progress, optimizing your model, and handling your data.\n",
    "\n",
    "This part of the exercise will require you to complete the missing code in the ```Solver``` class and to train your model end to end. Therefore, have a look at the code in the file ```exercise_code/solver.py```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implement the Solver\n",
    "\n",
    "Open the file `exercise_code/solver.py` and have a look at the ```Solver``` class. The ```_step()``` function is representing one single training step. So when using the Gradient Descent method, it represents one single update step using the Gradient Descent method. Your task is now to finalize this ```_step()``` function.\n",
    "\n",
    "Note here that our Solver takes as input our classifier model, the training and validation data and our loss function. Furthermore, we need to choose a learning rate. The ```_step()``` function is now a combination of the methods that you have been seen so far in this notebook. You will need to use the ```forward()``` and ```backward()``` method of your model and your loss function as well as the ```step()``` function of your optimizer. \n",
    "\n",
    "\n",
    "***Hint:*** The implementation of the ```_step()``` function is very similar to the implementation of a training step as we observed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> TESTING TEAM: Insert a test to check whether the implementation of step() method is correct. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.utils import test_accuracy\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "# Select the number of features, you want your task to train on.\n",
    "# Feel free to play with the sizes.\n",
    "num_features = 1\n",
    "\n",
    "# initialize model and weights\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "\n",
    "if np.shape(X_val)[1]==1:\n",
    "    plt.scatter(X_val, y_val, label = \"Ground Truth\")\n",
    "    inds = X_test.flatten().argsort(0)\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "#We use the BCE loss\n",
    "loss = BCE()\n",
    "\n",
    "# Please use these hyperparmeter as we also use them later in the evaluation\n",
    "learning_rate = 1e-1\n",
    "epochs = 25000\n",
    "\n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training.\n",
    "solver = Solver(model, \n",
    "                data, \n",
    "                loss,\n",
    "                learning_rate, \n",
    "                verbose=True, \n",
    "                print_every = 1000)\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process losses in each epoch are stored in the lists solver.train_loss_history and solver.val_loss_history. We can use them to plot the training result easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    "# Test final performance\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save your BCELoss, Classifier and Solver for Submission\n",
    "\n",
    "Your model should be trained now and able to predict whether a house is expensive or not. Hooooooray! The model will be saved as a pickle file to `models/logistic_regression.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \n",
    "        \"BCE_class\": BCE,\n",
    "        \"Classifier_class\": Classifier,\n",
    "        \"Solver_class\": Solver\n",
    "    },\n",
    "    file_name=\"logistic_regression.p\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
