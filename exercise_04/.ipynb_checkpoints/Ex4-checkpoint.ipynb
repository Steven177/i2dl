{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classifier / Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having worked with the Dataloading part last week, we want to start this week to take a more detailed look into how the training process looks like. So far, our tools are limited and we must restrict ourselves to a simplified model. But nevertheless, this gives us the opportunity to look at the different parts of the training process in more detail and builds up a good base when we turn to more complicated model architectures in the next exercises. \n",
    "\n",
    "This notebook will demonstrate a simple logistic regression model predicting whether a house is ```low-priced``` or ```expensive```. The data that we will use here is the HousingPrice dataset. Feeding some features in our classifier, the output should then be a score that determines in which category the considered house is.\n",
    "\n",
    "![classifierTeaser](images/classifierTeaser.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us first import some libraries and code that we will need along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Dataloading and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we apply preprocessing steps from ```house-prices-data-exploration.ipynb```. In machine learning, it is always important that any preprocessing step we apply on the training data is also applied on the validation and test data. \n",
    "\n",
    "Before we start preprocessing our data, let us first download the dataset and use the ```CSVDataset``` class to access our downloaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "target_column = 'SalePrice'\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the dataset in i2dl_exercises/datasets/housing in your file browser, which should contain a csv file containing all the data. \n",
    "\n",
    "It is always a good idea to get an overview of how our dataset looks like. You can see by executing the following cell some data samples from our dataset which is containing for each house many features. In total, our dataset provides for each house 81 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our classifier is a very simple version we restrict our model to only a few of the given features. In our case, let us select one feature which will be the column ```GrLivArea``` and use this one to predict the target column which we choose to be the column ```SalePrice```. This setting has the advantage that we can easiliy visualize our data in a 2 dimensional setting. Of course, a greater choice of features would make our model more powerful and accurate. But as we said, we want to keep it simple here and focus on the training process. The data that we will need will then reduce to the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected feature and target \n",
    "train_dataset.df[['GrLivArea',target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to use ‘GrLivArea’ as a feature, since it indicates the above ground living area available in the house, which correlates to our target intutively. Using a scatter plot, we can visualize the relationship between ‘GrLivArea’ and 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_dataset.df[['GrLivArea']], train_dataset.df[[target_column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are at very different scales and variances. Therfore, we normalize the features ranges with the minimum and maximum value of each numeric column. For filling in missing numeric values (if any), we need the mean value. These values should be pre-computed on the training set and used for all dataset splits. \n",
    "\n",
    "The ```FeatureSelectorAndNormalizationTransform``` class defined in ```exercise_code/data/csv_dataset.py``` is implementing this transformation. Make sure you have a look at the code of this file to understand the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having computed the ```min```, ```max``` and ```mean``` value, we load the data splits and perform the transformation on our data using the ```CSVDataset``` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the respective data splits ('train', 'val, and 'test') into one matrix of shape $N \\times D$ where $N$ represents the number of samples and $D$ the number of features (in our case we only have one feature). Similarly, we load the target data in one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into a matrix of shape (N, D), same for targets resulting in the shape (N, 1)\n",
    "X_train = [train_dataset[i]['features'] for i in range((len(train_dataset)))]\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = [train_dataset[i]['target'] for i in range((len(train_dataset)))]\n",
    "y_train = np.stack(y_train, axis=0)\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "\n",
    "# load validation data\n",
    "X_val = [val_dataset[i]['features'] for i in range((len(val_dataset)))]\n",
    "X_val = np.stack(X_val, axis=0)\n",
    "y_val = [val_dataset[i]['target'] for i in range((len(val_dataset)))]\n",
    "y_val = np.stack(y_val, axis=0)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "\n",
    "# load train data\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "print(\"test data shape:\", X_val.shape)\n",
    "print(\"test targets shape:\", y_val.shape)\n",
    "\n",
    "\n",
    "# 0 encodes small prices, 1 encodes large prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we model the regression task as a binary classification problem in the categories ```low-priced``` and ```expensive``` by labeling the 30% of the houses that are sold with the lowest price with ```0``` and, accordingly, the 30% of the houses with the highest price with ```1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.utils import binarize\n",
    "y_all = np.concatenate([y_train, y_val, y_test])\n",
    "thirty_percentile = np.percentile(y_all, 30)\n",
    "seventy_percentile = np.percentile(y_all, 70)\n",
    "\n",
    "# Prepare the labels for classification.\n",
    "X_train, y_train = binarize(X_train, y_train, thirty_percentile, seventy_percentile )\n",
    "X_val, y_val   = binarize(X_val, y_val, thirty_percentile, seventy_percentile)\n",
    "X_test, y_test  = binarize(X_test, y_test, thirty_percentile, seventy_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have preprocessed our data and we are now good to go to use the data for our training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a classfier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ be our data with $N$ samples and $D$ feature dimensions. We want to predict binary labels $\\mathbf{y} \\in \\mathbb{R}^{N\\times 1}$ for the samples. We want to estimate them with a simple classifier of the form\n",
    "\n",
    "$$ \\mathbf{y}  = \\sigma \\left( \\mathbf{X} \\mathbf{w} \\right), $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight of our classifier. The sigmoid function $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n",
    "\n",
    "$$ \\sigma(t) = \\frac{1}{1+\\mathrm{exp}(-t)}, $$\n",
    "\n",
    "is used to squash the ouputs of the linear layer into the interval $[0, 1]$ . It allows us to intepret the ouput of the neural network as probability. and we can compute the label predictions by rounding the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Where is the code to define our model here? Furthermore, please check the text above, there is a typo. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss: Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification like our task, a typical loss function is binary cross entropy. \n",
    "$$BCE(y,\\hat{y}) =-\\hat y log(y) - (1- \\hat y) log(1-y) $$ $ $\n",
    "where $y\\in\\mathbb{R}$ is the ground truth, and $\\hat y\\in\\mathbb{R}$ is the predicted probability of the house being expensive.\n",
    "\n",
    "Unlike linear regression, there is no closed-form solution for the optimal weights vector in logistic regression, since the BCE funcion is not convex. In order to find the optimal parameter, we need to use numeric methods such as gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to implement your cross entropy loss function  in `exercise_code/networks/loss.py` . You need to write the forward and backward pass of BCE as `forward` and `backward` function. The backward pass of the loss is needed to later optimize your weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Again, what happens in this cell? At least mention it in a short sentence.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Forward and Backward Check\n",
    "\n",
    "Once you have finished implementation of BCE loss class, you can run the following code to check whether your forward result and backward gradient are correct. You should expect your relative error to be lower than 1e-8.\n",
    "\n",
    "Here we will use a numeric gradient check to debug the backward pass:\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "where $h$ is a very small number, in practice approximately 1e-5 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "print (BCETest(bce_loss)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> I don't think that we should merge the backpropagation pass and the gradient descent step. In the old notebooks, this has been not merged too and I like that the students first have to implement the forward and the backward pass and then we have the optimizer step whoch essentially is super easy.\n",
    "\n",
    "As I mentioned in my comments, I would like this section to be very detailed. Please try to explain the task carefully and include some good explanation for the calculation of the backward pass making sure that the dimensions are correct. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, here we need a part to test their implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> This introduction for the optimizer part is very short and I would prefer to have it similar to the old notebook. Can you change it here to a more detailed explanation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the loss function $L(w)$ parameterized by the model parameters $w \\in \\mathbb{R}^{(D+1) \\times 1}$, we use an optimizer to update our model parameters <font color='red'> when training data models.  </font>\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Initialize the weights with random values. \n",
    "1. Calculate loss with the current weights and the loss function.\n",
    "2. Calculate the gradient of the loss function w.r.t. the weights\n",
    "3. Update weights with the corresponding gradient\n",
    "4. Iteratively perform Step 1 to 3 until converges\n",
    "\n",
    "$\\textbf{Gradient Descent}$\n",
    "\n",
    "Gradient descent is one of the most popular optimization strategy used for training Neural Networks. The weights are updated each step as following:\n",
    "$$ w_{n+1} = w_{n} - \\alpha \\frac {dL}{dw}, $$\n",
    "$ $ where $ \\frac {dL}{dw}$ is the gradient of your loss function w.r.t. the weight, and $\\alpha$ is the learning rate which is a predefined positive scalar determining the size of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implement a Naive Optimizer using Gradient Descent \n",
    "\n",
    "Here we will use gradient descent method to update our loss function to see how it changes when updating our weights in the model. Open the file `exercise_code/networks/optimizer.py` and implement the gradients descent step into the `step()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Where is the code here?? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to show you to how the sequence of forward passing, compute the model loss and gradients, and backward passing to update the parameters, can be implemented as iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Please explain here as well more of what is happening. We do the training and furthermore we also record the loss.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import *\n",
    "from exercise_code.networks.classifier import *\n",
    "# Hyperparameter Setting, we will specify the loss function we use, and implement the optimizer we finished in the last step.\n",
    "num_features = 1\n",
    "\n",
    "# initialization\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "loss_func = BCE() \n",
    "learning_rate = 5e-1\n",
    "loss_history = []\n",
    "opt = Optimizer(model,learning_rate)\n",
    "\n",
    "steps = 100\n",
    "# Full batch Gradient Descent\n",
    "for i in range(steps):\n",
    "    \n",
    "    # Enable your model to store the gradient.\n",
    "    model.train()\n",
    "    \n",
    "    # Compute the output and gradients w.r.t weights of your model for the input dataset.\n",
    "    model_forward, model_backward = model(X_train)\n",
    "    \n",
    "    # Compute the loss and gradients w.r.t output of the model.\n",
    "    loss, loss_grad = loss_func(model_forward, y_train)\n",
    "    \n",
    "    # Use back prop method to get the gradients of loss w.r.t the weights.\n",
    "    grad = loss_grad * model_backward\n",
    "    \n",
    "    # Compute the average gradient over your batch\n",
    "    grad = np.mean(grad, 0, keepdims = True)\n",
    "\n",
    "    # After obtaining the gradients of loss with respect to the weights, we can use optimizer to\n",
    "    # do gradient descent step.\n",
    "    opt.step(grad.T)\n",
    "    \n",
    "    # Average over the loss of the entire dataset and store it.\n",
    "    average_loss = np.mean(loss)\n",
    "    loss_history.append(average_loss)\n",
    "    if i%10 == 0:\n",
    "        print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Please explain every cell, what happens here? The explanation at the end can be better given beforehand and could be a little bit more detailed. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history to see how it goes after several steps of gradient descent.\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can observe that our loss decreases and therefore our model improves to explain the underlying relationship of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to put everything we have learned so far together in an organized and concise way, that provides easy access to train a network/model in your own script/code. We will define a Solver class to provides an abstraction for all the gritty details behind training your parameters.\n",
    "\n",
    "A Solver should be able to train a network with training data by graident descent, while monitoring overfitting with the validataion data along the training process.\n",
    "\n",
    "To create a Solver instance, we need a model object, dataset, learning_rate as the input arguments, then the model can be easily trained with just a call to the train() method. At the end, the optimal weight will be stored according to the loss on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, you can have a look at how bad the prediction is with random initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.utils import test_accuracy\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "\n",
    "# Select the number of features, you want your task to train on.\n",
    "# Feel free to play with the sizes.\n",
    "num_features = 1\n",
    "\n",
    "# initialize model and weights\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implement the update step of gradient descent\n",
    "\n",
    "In the function `_step()` a single step of gradient descent is performed. Recall in the previous section `Training`, you have seen gradient descent in details. Here we want to integrate this part into our Solver. Open `exercise_code/solver.py` and implement **A SINGLE UPDATE STEP** of gradient descent in `_step()`.\n",
    "\n",
    "#### Hints: (too many hints?)\n",
    "\n",
    "Step 1~3 in the procedure are counted as a single update step. You may need the `backward()` or `__call()__` from your model to compute the gradients of the loss w.r.t. the weight, and `step()` from optimizer to update your weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> What happens now in this cell? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "loss = BCE()\n",
    "\n",
    "# Please use these hyperparmeter as we also use them later in the evaluation\n",
    "learning_rate = 1e-1\n",
    "epochs = 25000\n",
    "\n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training.\n",
    "solver = Solver(model, \n",
    "                data, \n",
    "                loss,\n",
    "                learning_rate, \n",
    "                verbose=True, \n",
    "                print_every = 1000)\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process losses in each epoch are stored in the lists solver.train_loss_history and solver.val_loss_history. We can use them to plot the training result easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    "# Test final performance\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save your BCELoss, Classifier and Solver for Submission\n",
    "\n",
    "Your model should be trained now and able to predict whether a house is expensive or not. Hooooooray! The model will be saved as a pickle file to `models/logistic_regression.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \n",
    "        \"BCE_class\": BCE,\n",
    "        \"Classifier_class\": Classifier,\n",
    "        \"Solver_class\": Solver\n",
    "    },\n",
    "    file_name=\"logistic_regression.p\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
