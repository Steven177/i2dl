{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classifier / Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demosntrate a simple logistic regression model predicting whether a house is ```low-priced``` or ```expensive```. Similar to our linear model in ```1_linear_regression.ipynb```, we feed features from the HousingPrice dataset into the classifier model. However, now, we expect our model to output a score that determines in which category the considered house is. \n",
    "![classifierTeaser](images/classifierTeaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ denote our data with $N$ samples and $D$ feature dimensions. Our targets, the binary labels, are given by $\\mathbf{y} \\in \\mathbb{R}^{N\\times 1}$. We want to estimate them with a simple classifier of the form\n",
    "\n",
    "$$ \\mathbf{y}  = \\sigma \\left( \\mathbf{X} \\mathbf{w} \\right), $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight of our classifier. The sigmoid function $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n",
    "\n",
    "$$ \\sigma(t) = \\frac{1}{1+\\mathrm{exp}(-t)}, $$\n",
    "\n",
    "is used to squeeze the ouputs of the linear layer into the range $[0, 1]$. This provides us with a probabilistic interpretation of the ouput of the neural network and we can compute the label predictions by rounding the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same dataloading and preprocessing steps as in the notebook ```1_linear_regression.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data transformations, compute min, max and mean for each feature column. We perform the same transformation on the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "# Select only 2 features to keep plus the target column.\n",
    "#selected_columns = ['OverallQual', 'GrLivArea', target_column]\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into a matrix of shape (N, D), same for targets resulting in the shape (N, 1)\n",
    "X_train = [train_dataset[i]['features'] for i in range((len(train_dataset)))]\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = [train_dataset[i]['target'] for i in range((len(train_dataset)))]\n",
    "y_train = np.stack(y_train, axis=0)\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "\n",
    "# load validation data\n",
    "X_val = [val_dataset[i]['features'] for i in range((len(val_dataset)))]\n",
    "X_val = np.stack(X_val, axis=0)\n",
    "y_val = [val_dataset[i]['target'] for i in range((len(val_dataset)))]\n",
    "y_val = np.stack(y_val, axis=0)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "\n",
    "# load train data\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "print(\"test data shape:\", X_val.shape)\n",
    "print(\"test targets shape:\", y_val.shape)\n",
    "\n",
    "\n",
    "# 0 encodes small prices, 1 encodes large prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we model the regression task as a binary classification problem in the categories ```low-priced```and ```expensive``` by labeling the 30% of the houses that are sold with the lowest price with ```0``` and, accordingly, the 30% of the houses with the highest price with ```1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.utils import binarize\n",
    "y_all = np.concatenate([y_train, y_val, y_test])\n",
    "thirty_percentile = np.percentile(y_all, 30)\n",
    "seventy_percentile = np.percentile(y_all, 70)\n",
    "\n",
    "# Prepare the labels for classification.\n",
    "X_train, y_train = binarize(X_train, y_train, thirty_percentile, seventy_percentile )\n",
    "X_val, y_val   = binarize(X_val, y_val, thirty_percentile, seventy_percentile)\n",
    "X_test, y_test  = binarize(X_test, y_test, thirty_percentile, seventy_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a classfier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple classifier in ```exercise_code/networks/classifier.py```. Implement the forward pass in method ```forward()``` and the backward pass in ```backward()``` in the Network class ```Classifier```. This time, you also need to implement the function ```sigmoid()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "model = Classifier(num_features=1)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement the Loss Function: Binary Cross Entropy\n",
    "\n",
    "\n",
    "In this part, you will implement a binary cross entropy (BCE) loss function. Open the file `exercise_code/networks/loss.py` and implement the forward and backward pass of BCE loss into the `forward` and `backward` function.\n",
    "\n",
    "Remember the BCE loss function is:\n",
    "$$ bce = -\\hat y log(y) - (1- \\hat y) log(1-y)$$\n",
    "\n",
    "$ $ where $y$ is the output of your model, and $\\hat y$ is the ground truth of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Check\n",
    "\n",
    "Once you have finished implementation of BCE loss class, you can run the following code to check whether your forward result and backward gradient are correct. You should expect your relative error to be lower than 1e-8.\n",
    "\n",
    "Here we will use a numeric gradient check to debug the backward pass:\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "where $h$ is a very small number, in practice approximately 1e-5 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "print (BCETest(bce_loss)())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Solver\n",
    "\n",
    "You have successfully implement a solver in the last task, now we will use that solver to solve this logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.utils import test_accuracy\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "\n",
    "# Select the number of features, you want your task to train on.\n",
    "# Feel free to play with the sizes.\n",
    "num_features = 1\n",
    "\n",
    "# initialize model and weights\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "\n",
    "if np.shape(X_val)[1]==1:\n",
    "    plt.scatter(X_val, y_val, label = \"Ground Truth\")\n",
    "    inds = X_test.flatten().argsort(0)\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "# We are going to use the BCE loss for this task.\n",
    "loss = BCE()\n",
    "\n",
    "# Please use these hyperparmeter as we also use them later in the evaluation\n",
    "learning_rate = 1e-1\n",
    "epochs = 25000\n",
    "\n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training.\n",
    "solver = Solver(model, \n",
    "                data, \n",
    "                loss,\n",
    "                learning_rate, \n",
    "                verbose=True, \n",
    "                print_every = 1000)\n",
    "\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)\n",
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    "# Test final performance\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your BCELoss, Classifier and Solver for Submission\n",
    "Simply save your objects using the following cell. This will save them to a pickle file `models/logistic_regression.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \n",
    "        \"BCE_class\": BCE,\n",
    "        \"Classifier_class\": Classifier,\n",
    "        \"Solver_class\": Solver\n",
    "    },\n",
    "    file_name=\"logistic_regression.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the neccessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Navigate to `exercise_code` directory and run the `create_submission.sh` file to create the zip file of your model. This will create a single `zip` file that you need to upload. Otherwise, you can also zip it manually if you don't want to use the bash script.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"dummy_model.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a classifier, a BCE loss function and a solver that can perform gradient descent and finally the model can predict the given dataset with an accuracy higher than 85%.\n",
    "- Test cases:\n",
    "  1. Does `forward()` and `backward()` of your classifier return the correct value and data type?\n",
    "  2. Does `forward()` and `backward()` of your BCE loss return the correct value and data type?\n",
    "  3. Does your `solver.train()` train the model that it achieves a prediction accuracy of your model beyond the given threshold accuracy of 85%? We train your classifier model with new initialised weights, lr = 0.1 and 25000 epochs on a 1-D classification problem.\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 33.3 per passed test\n",
    "- Threshold to clear exercise: 80\n",
    "- Submission start: __May 14, 2020 12.00__\n",
    "- Submission deadline : __May 20, 2020 23.59__ \n",
    "- You can make multiple submission uptil the deadline. Your __best submission__ will be considered for bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
