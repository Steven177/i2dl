{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook demonstrate a generic training pipeline to train a machine learning model. For this, we use a simple linear regression model to regress house prices.\n",
    "\n",
    "![teaser](images/teaser.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercse is to implement a linear model with forward and backward pass to regresses the housing prices based on the dataset HousingPrices. Have a look at the ```house-prices-data-exploration.ipynb``` from last week's exercise to get a nice overview of the dataset. The notebook also explains the dataloading and pre-processing steps that we will use in this exercise for dataloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ A Linear Regression model is defined as follows:\n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times D}$ denote our data with $N$ samples and $D$ feature dimensions. Our targets, the housing prices, are given by $\\mathbf{y} \\in \\mathbb{R}^{N\\times 1}$. We want to estimate them with a linear model that predicts the price at which a house was sold based on a set of features, i.e., a model of the form\n",
    "\n",
    "$$ \\hat{y_{i}}  = \\mathbf{x}_i \\cdot \\mathbf{w} + b, $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{D \\times 1}$ is the weight of our linear model, $b$ the bias, and the index $i$ denotes one sample. If we extend the our samples with a column of 1s $(\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)})$, we can absorb the bias into the weight $\\mathbf{w} \\in \\mathbb{R}^{(D+1) \\times 1}$ (note the +1 in the feature dimension), such that we only have a matrix multiplication resulting in \n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{X} \\mathbf{w} $$\n",
    "\n",
    "$ $ over all $N$ samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see an example of a 1-D linear regression.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader  import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we apply preprocessing steps from ```house-prices-data-exploration.ipynb```. In machine learning, it is always important that any preprocessing step we apply on the training data is also applied on the validation and test data. \n",
    "\n",
    "The features are at very different scales and variances. Therfore, we normalize the features ranges with the minimum and maximum value of each numeric column. For filling in missing numeric values (if any), we need the mean value. These values we should pre-compute on the training set and feed them to the transform that is used on all dataset splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ For means of visualization, we only consider number of features $D=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"SalePrice\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute min, max and mean for each feature column for the transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "target_column = 'SalePrice'\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the same transformation on the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ We load the data into one matrix of shape $(N, D)$, similar for the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into a matrix of shape (N, D), same for targets resulting in the shape (N, 1)\n",
    "X_train = [train_dataset[i]['features'] for i in range((len(train_dataset)))]\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = [train_dataset[i]['target'] for i in range((len(train_dataset)))]\n",
    "y_train = np.stack(y_train, axis=0)\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "\n",
    "# load validation data\n",
    "X_val = [val_dataset[i]['features'] for i in range((len(val_dataset)))]\n",
    "X_val = np.stack(X_val, axis=0)\n",
    "y_val = [val_dataset[i]['target'] for i in range((len(val_dataset)))]\n",
    "y_val = np.stack(y_val, axis=0)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "\n",
    "# load test data\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ We define a linear model in ```exercise_code/networks/linear_model.py```. \n",
    "Your task is now to implement the forward pass in method ```forward()``` and the backward pass in ```backward()``` in the Network class ```LinearModel```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.linear_model import LinearModel\n",
    "\n",
    "model = LinearModel(num_features=1)\n",
    "\n",
    "# choose weights for initialization\n",
    "weights = np.array([[0.8, 0]]).transpose()\n",
    "model.initialize_weights(weights)\n",
    "\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the result of the forward pass in the following. Note that we choose the weights for the initialization of our model. As you can see, by choosing a good prior, you can already get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, np.squeeze(y_out), color='r')\n",
    "plt.xlabel('GrLivArea');\n",
    "plt.ylabel('SalePrice');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $ Since we normalized our dataset, the predictions are still in the range $[0, 1]$. Let's scale them back to the original range with min, max and mean from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions by resacling the predicted values\n",
    "X_rescaled = rescale(X_train, key = \"GrLivArea\")\n",
    "y_rescaled = rescale(y_out, key = \"SalePrice\")\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(df['GrLivArea'], df['SalePrice'])\n",
    "plt.plot(X_rescaled, y_rescaled, color='r')\n",
    "plt.xlabel('GrLivArea');\n",
    "plt.ylabel('SalePrice');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the aforementioned visualisation we already initialised the weigths of model wiith a good guess. But what do we have to do, if we do not know the model weights yet. For linear regression we can solve the problem analytically. However, this is not possible for more complex models such as neural networks. Therefore, we have to compute a solution nummericially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Gradient Descent\n",
    "\n",
    "In order to train our model we will discuss the following steps in this exercise:\n",
    "\n",
    "- Implement a **loss function** for your model\n",
    "- **Compute the gradient** of your loss function\n",
    "- **Check your implementation** with numerical gradient\n",
    "- **Optimize** the loss function with **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply and train you model you have to first define a loss or objective fucntion to which respect you want to optimize your model to. For the task of regression, we usually consider  $ L_{1}$, \n",
    "$$ L_{1} = |y - \\hat y|,  $$\n",
    "and mean squared error (MSE), \n",
    "MSE loss function is:\n",
    "$$ MSE = (y - \\hat y)^2,  $$\n",
    "$ $ where $y$ is the output of your model, and $\\hat y$ is the ground truth of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to implement your loss function in `exercise_code/networks/loss.py` and write the forward and backward pass of $ L_{1}$ and MSE as `forward` and `backward` function. The backward pass of the loss is needed to later optimize your weights of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented you loss function you can import your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.loss import L1, MSE\n",
    "\n",
    "l1_loss = L1()\n",
    "mse_loss = MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Check\n",
    "\n",
    "Once you have finished implementation of L1 and MSE loss class, you can run the following code to check whether your forward result and backward gradient are correct. You should expect your relative error to be lower than 1e-8. (Note that gradients at 0 of $ L_{1}$ loss is also 0! )\n",
    "\n",
    "Here we will use a numeric gradient check to debug the backward pass:\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "where $h$ is a very small number, in practice approximately 1e-5 or so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "print (L1Test(l1_loss)())\n",
    "print (MSETest(mse_loss)())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the tests were successful, you can continue with implementing gradient descent and your optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Optimizer and Gradient Descent\n",
    "\n",
    "Previously, we have successfully dealt with the loss function, which is a mathematical tool for predicting the prediction effect.\n",
    "\n",
    "During the training process, we will adjust the parameters (weights) of the model to try to minimize the loss function and try to optimizer the predictions of our model.\n",
    "\n",
    "This is where the optimizer comes in. They update the model with respect to the output of the loss function, thereby linking the loss function and model parameters together. In short, the optimizer shapes and models the most accurate form by updating weights. The loss function is a guide to the terrain and can tell the optimizer when to move in the right or wrong direction.\n",
    "\n",
    "Any discussion about optimizers needs to begin with the most popular one, and it’s called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It’s fast, robust, and flexible. Here’s how it works:\n",
    "\n",
    "1. Calculate the gradient in each individual weight would do to the loss function;\n",
    "2. Ubdate each weight based on its gradient;\n",
    "3. Iterativily doing step 1 and step 2 till convergence.\n",
    "\n",
    "Gradient descent follows the following mathematical form:\n",
    "\n",
    "$$ W = W - \\alpha \\frac {dL}{dW}, $$\n",
    "$ $ where $W$ is weight of your model, $\\alpha$ is the learning rate you need to set before training (we will discuss that more in the comming lectures), $ \\frac {dL}{dW}$ is the gradients of your loss function with respect to the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Naive Optimizer using Gradient Descent \n",
    "\n",
    "Here we will use gradient descent method to update our loss function to see how it changes when updating our weights in the model. Open the file `exercise_code/networks/optimizer.py` and implement the gradients descent step into the `step()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import *\n",
    "\n",
    "X_train = X_train\n",
    "X_val = X_val\n",
    "# Implement Linear Model and initialize the weights.\n",
    "model = LinearModel(num_features=1)\n",
    "model.initialize_weights()\n",
    "\n",
    "print(np.shape(X_train))\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the prediciton of the model are really bad when we randomly initialise the weights. Let's see how this changes, when we apply gradient descent to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Setting, we will specify the loss function we use, and implement\n",
    "# the optimizer we finished in the last step.\n",
    "loss_func = MSE() \n",
    "learning_rate = 5e-1\n",
    "loss_history = []\n",
    "opt = Optimizer(model,learning_rate)\n",
    "steps = 100\n",
    "# Full batch Gradient Descent\n",
    "for i in range(steps):\n",
    "    \n",
    "    # Enable your model to store the gradient.\n",
    "    model.train()\n",
    "    \n",
    "    # Compute the output and gradients with repect to weight of your model for the input dataset.\n",
    "    model_forward,model_backward = model(X_train)\n",
    "    \n",
    "    # Compute the loss and gradients with repect to output of the model.\n",
    "    loss, loss_grad = loss_func(model_forward, y_train)\n",
    "    \n",
    "    # Use back prop method to get the gradients of loss with respect to the weights.\n",
    "    grad = loss_grad * model_backward\n",
    "    \n",
    "    #Compute the average gradient over your batch\n",
    "    grad = np.mean(grad, 0, keepdims = True)\n",
    "\n",
    "    # After obtaining the gradients of loss with respect to the weights, we can use optimizer to\n",
    "    # do gradient descent step.\n",
    "    opt.step(grad.T)\n",
    "    \n",
    "    # Average over the loss of the entire dataset and store it.\n",
    "    average_loss = np.mean(loss)\n",
    "    loss_history.append(average_loss)\n",
    "    print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n",
    "\n",
    "# Plot the loss history to see how it goes after several steps of \n",
    "# gradient descent.\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our loss decreases and the linear model improves to ecplain the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solver\n",
    "\n",
    "Now that you've learned how to build your own neural regressors and classifiers, let's put everything together.\n",
    "This part of the exercise will require you to complete the missing code in the 'Solver' class, and use that to train your models end to end.\n",
    "\n",
    "The purpose of a solver is to mainly to provide an abstraction for all the gritty details behind training your parameters, such as logging your progress, optimizing your model, and handling your data.\n",
    "\n",
    "In order to explore the full generality of the solver, we will address two common tasks in machine learning of machine learning, namely classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Solver\n",
    "\n",
    "Open the file `exercise_code/solver.py` and finalize the `_step()`function. \n",
    "\n",
    "Note that we will initialize our solver with given training and validation set and perform loss update by calling `_step()`, here you may use the `backward()` or `__call()__` function in your loss and model to calculate the gradients of the loss with respect to the weight, and finally perform gradient descent with the help of `step()` function in your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from exercise_code import solver\n",
    "\n",
    "# Select the number of features, you want your task to train on.\n",
    "# Feel free to play with the sizes.\n",
    "num_features = 1\n",
    "\n",
    "# Use a simple linear model to perform linear regression\n",
    "# on real-valued labels.\n",
    "model = LinearModel(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "# Build the actual dataset.\n",
    "# Notice how we use an 80-20 train validation split.\n",
    "# You're welcome to experiment with other splits\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "# We are going to use the L2 loss for this task, but L1 \n",
    "# is also an appropriate loss function for this task.\n",
    "l1_loss = L1()\n",
    "mse_loss = MSE()\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 25000\n",
    "\n",
    "print(\"L1 loss on test set BEFORE training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "print(\"MSE loss on test set BEFORE training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training.\n",
    "solver = solver.Solver(model, data, mse_loss,\n",
    "                       learning_rate, verbose=True, print_every= 1000)\n",
    "\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)\n",
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    "\n",
    "# Test final performance\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "\n",
    "print(\"L1 loss on test set AFTER training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "print(\"MSE loss on test set AFTER training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play around with the different loss functions and use more than one feature to do the regression. Does the result improve? Note that if you want to add more features you need to build your own testing set with correspoing dimensions of the features, since here our testing set has only one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this notebook is running and you have understood everything what is in there you can go to next notebook `2_logistic_regression.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
