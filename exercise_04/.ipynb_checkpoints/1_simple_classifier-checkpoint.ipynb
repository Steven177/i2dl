{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classifier / Logistic Regression\n",
    "\n",
    "After having worked with the Dataloading part last week, we want to start this week to take a more detailed look into how the training process looks like. So far, our tools are limited and we must restrict ourselves to a simplified model. But nevertheless, this gives us the opportunity to look at the different parts of the training process in more detail and builds up a good base when we turn to more complicated model architectures in the next exercises. \n",
    "\n",
    "This notebook will demonstrate a simple logistic regression model predicting whether a house is ```low-priced``` or ```expensive```. The data that we will use here is the HousingPrice dataset. Feeding some features in our classifier, the output should then be a score that determines in which category the considered house is.\n",
    "\n",
    "![classifierTeaser](images/classifierTeaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us first import some libraries and code that we will need along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataloading and Data Preprocessing\n",
    "\n",
    "Let us load the data that we want to use for our training. The method `get_housing_data()` is providing you with a training, validation and test set that is ready to use.\n",
    "\n",
    "For more information about how to prepare the data and what the final data look like, you can have a look at the notebook `housing_data_preprocessing(optional).ipynb `. We reduced our data and the remaining houses in our dataset are now either labeled with ```1``` and hence categorized as ```expensive```, or they are labeled with ```0``` and hence categorized as ```low-priced```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You successfully loaded your data! \n",
      "\n",
      "train data shape: (533, 1)\n",
      "train targets shape: (533, 1)\n",
      "val data shape: (167, 1)\n",
      "val targets shape: (167, 1)\n",
      "test data shape: (177, 1)\n",
      "test targets shape: (177, 1) \n",
      "\n",
      "The original dataset looks as follows:\n",
      "{'features': array([0.41088924], dtype=float32), 'target': array([0.23338121])}\n",
      "[[0.41088924]\n",
      " [0.23436323]\n",
      " [0.16371515]\n",
      " [0.14280331]\n",
      " [0.12961568]\n",
      " [0.21288621]\n",
      " [0.78033155]\n",
      " [0.19178598]\n",
      " [0.31066316]\n",
      " [0.1066315 ]\n",
      " [0.14091937]\n",
      " [0.12358704]\n",
      " [0.09080633]\n",
      " [0.05576488]\n",
      " [0.2569706 ]\n",
      " [0.16616428]\n",
      " [0.27166542]\n",
      " [0.271477  ]\n",
      " [0.35116804]\n",
      " [0.32253203]\n",
      " [0.2311605 ]\n",
      " [0.10324039]\n",
      " [0.33157498]\n",
      " [0.43104747]\n",
      " [0.34363225]\n",
      " [0.19329314]\n",
      " [0.28296912]\n",
      " [0.4634514 ]\n",
      " [0.26262245]\n",
      " [0.09250189]\n",
      " [0.10550113]\n",
      " [0.11228335]\n",
      " [0.43971363]\n",
      " [0.08553127]\n",
      " [0.2467973 ]\n",
      " [0.10135645]\n",
      " [0.20685758]\n",
      " [0.2763753 ]\n",
      " [0.08571967]\n",
      " [0.13451394]\n",
      " [0.16993217]\n",
      " [0.11190656]\n",
      " [0.25584024]\n",
      " [0.26356444]\n",
      " [0.1399774 ]\n",
      " [0.11492088]\n",
      " [0.20440844]\n",
      " [0.1179352 ]\n",
      " [0.24717408]\n",
      " [0.12528259]\n",
      " [0.11322532]\n",
      " [0.4009043 ]\n",
      " [0.19084401]\n",
      " [0.5184627 ]\n",
      " [0.16578749]\n",
      " [0.1948003 ]\n",
      " [0.2539563 ]\n",
      " [0.287679  ]\n",
      " [0.2166541 ]\n",
      " [0.09984928]\n",
      " [0.26375282]\n",
      " [0.18952525]\n",
      " [0.07272042]\n",
      " [0.39242652]\n",
      " [0.11623964]\n",
      " [0.19235116]\n",
      " [0.06782216]\n",
      " [0.13093443]\n",
      " [0.14770158]\n",
      " [0.38771665]\n",
      " [0.13300678]\n",
      " [0.30614167]\n",
      " [0.12321025]\n",
      " [0.23624717]\n",
      " [0.21552373]\n",
      " [0.33666164]\n",
      " [0.17087415]\n",
      " [0.19027883]\n",
      " [0.09984928]\n",
      " [0.2535795 ]\n",
      " [0.1068199 ]\n",
      " [0.36266014]\n",
      " [0.17407687]\n",
      " [0.213263  ]\n",
      " [0.26714393]\n",
      " [0.21853806]\n",
      " [0.2577242 ]\n",
      " [0.27618688]\n",
      " [0.46156743]\n",
      " [0.15015072]\n",
      " [0.22287114]\n",
      " [0.09984928]\n",
      " [0.1179352 ]\n",
      " [0.23360965]\n",
      " [0.22287114]\n",
      " [0.3786737 ]\n",
      " [0.19291635]\n",
      " [0.12321025]\n",
      " [0.1507159 ]\n",
      " [0.4295403 ]\n",
      " [0.1838734 ]\n",
      " [0.14054258]\n",
      " [0.338734  ]\n",
      " [0.14770158]\n",
      " [0.258101  ]\n",
      " [0.0992841 ]\n",
      " [0.12302186]\n",
      " [0.22626224]\n",
      " [0.1729465 ]\n",
      " [0.05576488]\n",
      " [0.19423512]\n",
      " [0.08364733]\n",
      " [0.13300678]\n",
      " [0.4333082 ]\n",
      " [0.3426903 ]\n",
      " [0.27373776]\n",
      " [0.09984928]\n",
      " [0.24830444]\n",
      " [0.13300678]\n",
      " [0.08477769]\n",
      " [0.28409946]\n",
      " [0.3854559 ]\n",
      " [0.20290127]\n",
      " [0.4513941 ]\n",
      " [0.24905802]\n",
      " [0.05576488]\n",
      " [0.19932178]\n",
      " [0.24510173]\n",
      " [0.09984928]\n",
      " [0.22079879]\n",
      " [0.32253203]\n",
      " [0.3018086 ]\n",
      " [0.16993217]\n",
      " [0.13300678]\n",
      " [0.09984928]\n",
      " [0.28560662]\n",
      " [0.20308967]\n",
      " [0.09400904]\n",
      " [0.154107  ]\n",
      " [0.17897514]\n",
      " [0.27354935]\n",
      " [0.30237377]\n",
      " [0.09984928]\n",
      " [0.491899  ]\n",
      " [0.18462698]\n",
      " [0.28296912]\n",
      " [0.10022607]\n",
      " [0.5548229 ]\n",
      " [0.22155237]\n",
      " [0.27524492]\n",
      " [0.18085907]\n",
      " [0.32969102]\n",
      " [0.26978147]\n",
      " [0.44649586]\n",
      " [0.03504145]\n",
      " [0.32460436]\n",
      " [0.25621703]\n",
      " [0.21439336]\n",
      " [0.32893744]\n",
      " [0.35700828]\n",
      " [0.32969102]\n",
      " [0.2648832 ]\n",
      " [0.18877167]\n",
      " [0.31009796]\n",
      " [0.16654107]\n",
      " [0.2609269 ]\n",
      " [0.3896006 ]\n",
      " [0.07045969]\n",
      " [0.14638282]\n",
      " [0.28146195]\n",
      " [0.10550113]\n",
      " [0.25527507]\n",
      " [0.22739261]\n",
      " [0.12321025]\n",
      " [0.16654107]\n",
      " [0.13300678]\n",
      " [0.27128863]\n",
      " [0.1946119 ]\n",
      " [0.19329314]\n",
      " [0.3632253 ]\n",
      " [0.16691786]\n",
      " [0.32479277]\n",
      " [0.16277318]\n",
      " [0.24962321]\n",
      " [0.12773173]\n",
      " [0.3004898 ]\n",
      " [0.11680482]\n",
      " [0.28334588]\n",
      " [0.24604371]\n",
      " [0.15335342]\n",
      " [0.11605124]\n",
      " [0.29709873]\n",
      " [0.24510173]\n",
      " [0.18500377]\n",
      " [0.14807837]\n",
      " [0.19875659]\n",
      " [0.13733986]\n",
      " [0.27467972]\n",
      " [0.30614167]\n",
      " [0.1544838 ]\n",
      " [0.13451394]\n",
      " [0.10399397]\n",
      " [0.27015826]\n",
      " [0.4267144 ]\n",
      " [0.10889224]\n",
      " [0.10324039]\n",
      " [0.32818386]\n",
      " [0.41390353]\n",
      " [0.24321778]\n",
      " [0.34099472]\n",
      " [0.4598719 ]\n",
      " [0.2243783 ]\n",
      " [0.23304446]\n",
      " [0.08703844]\n",
      " [0.38394874]\n",
      " [0.43500376]\n",
      " [0.15015072]\n",
      " [0.30633005]\n",
      " [0.15259986]\n",
      " [0.06763376]\n",
      " [0.21853806]\n",
      " [0.24717408]\n",
      " [0.09984928]\n",
      " [0.23210248]\n",
      " [0.16465712]\n",
      " [0.1946119 ]\n",
      " [0.11699322]\n",
      " [0.1399774 ]\n",
      " [0.24114545]\n",
      " [0.32177845]\n",
      " [0.13922381]\n",
      " [0.17916353]\n",
      " [0.33590806]\n",
      " [0.05105501]\n",
      " [0.34853053]\n",
      " [0.23097211]\n",
      " [0.05576488]\n",
      " [0.3462698 ]\n",
      " [0.19065562]\n",
      " [0.42878672]\n",
      " [0.30595326]\n",
      " [0.2136398 ]\n",
      " [0.26375282]\n",
      " [0.16314997]\n",
      " [0.22908817]\n",
      " [0.3571967 ]\n",
      " [0.24585532]\n",
      " [0.09984928]\n",
      " [0.2424642 ]\n",
      " [0.12302186]\n",
      " [0.4116428 ]\n",
      " [0.19423512]\n",
      " [0.36850038]\n",
      " [0.42614922]\n",
      " [0.10795026]\n",
      " [0.16126601]\n",
      " [0.09871891]\n",
      " [0.3558779 ]\n",
      " [0.12641296]\n",
      " [0.13300678]\n",
      " [0.26883948]\n",
      " [0.19385833]\n",
      " [0.25527507]\n",
      " [0.33515447]\n",
      " [0.23342125]\n",
      " [0.3824416 ]\n",
      " [0.32027128]\n",
      " [0.09871891]\n",
      " [0.17407687]\n",
      " [0.01959307]\n",
      " [0.11680482]\n",
      " [0.25056517]\n",
      " [0.33911076]\n",
      " [0.53880936]\n",
      " [0.26789752]\n",
      " [0.12829691]\n",
      " [0.1507159 ]\n",
      " [0.24529013]\n",
      " [0.19027883]\n",
      " [0.10531273]\n",
      " [0.3012434 ]\n",
      " [0.16654107]\n",
      " [0.3308214 ]\n",
      " [0.24905802]\n",
      " [0.16767144]\n",
      " [0.3240392 ]\n",
      " [0.09796534]\n",
      " [0.14280331]\n",
      " [0.25      ]\n",
      " [0.25131875]\n",
      " [0.21834967]\n",
      " [0.11266013]\n",
      " [0.22532026]\n",
      " [0.18462698]\n",
      " [0.30670685]\n",
      " [0.06367747]\n",
      " [0.37792012]\n",
      " [0.24905802]\n",
      " [0.16804823]\n",
      " [0.14920874]\n",
      " [0.19951017]\n",
      " [0.5864732 ]\n",
      " [0.12302186]\n",
      " [0.19235116]\n",
      " [0.3268651 ]\n",
      " [0.30934438]\n",
      " [0.2315373 ]\n",
      " [0.14638282]\n",
      " [0.45101732]\n",
      " [0.18236624]\n",
      " [0.2241899 ]\n",
      " [0.23624717]\n",
      " [0.337792  ]\n",
      " [0.12019593]\n",
      " [0.31763375]\n",
      " [0.15259986]\n",
      " [0.19178598]\n",
      " [0.18500377]\n",
      " [0.29954785]\n",
      " [0.15636775]\n",
      " [0.38131124]\n",
      " [0.2535795 ]\n",
      " [0.16880181]\n",
      " [0.18217784]\n",
      " [0.2386963 ]\n",
      " [0.23624717]\n",
      " [0.1179352 ]\n",
      " [0.21137905]\n",
      " [0.26224566]\n",
      " [0.32931423]\n",
      " [0.39864355]\n",
      " [0.28805578]\n",
      " [0.08176338]\n",
      " [0.        ]\n",
      " [0.10889224]\n",
      " [0.16013564]\n",
      " [0.29295403]\n",
      " [0.24415976]\n",
      " [0.2245667 ]\n",
      " [0.13300678]\n",
      " [0.24736246]\n",
      " [0.31028637]\n",
      " [0.42200452]\n",
      " [0.28522983]\n",
      " [0.2172193 ]\n",
      " [0.10512434]\n",
      " [0.08854559]\n",
      " [0.15184627]\n",
      " [0.31518462]\n",
      " [0.06725697]\n",
      " [0.23662396]\n",
      " [0.27015826]\n",
      " [0.62038434]\n",
      " [0.2763753 ]\n",
      " [0.05576488]\n",
      " [0.14487566]\n",
      " [0.15900527]\n",
      " [0.16239639]\n",
      " [0.19555388]\n",
      " [0.3050113 ]\n",
      " [0.40297663]\n",
      " [0.13620949]\n",
      " [0.36473247]\n",
      " [0.31725696]\n",
      " [0.3050113 ]\n",
      " [0.3675584 ]\n",
      " [0.26073852]\n",
      " [0.29540315]\n",
      " [0.41070083]\n",
      " [0.42916352]\n",
      " [0.10700829]\n",
      " [0.38036925]\n",
      " [0.15523738]\n",
      " [0.19366993]\n",
      " [0.36868876]\n",
      " [0.18519217]\n",
      " [0.26375282]\n",
      " [0.21891485]\n",
      " [0.2392615 ]\n",
      " [0.0810098 ]\n",
      " [0.21853806]\n",
      " [0.12283346]\n",
      " [0.2166541 ]\n",
      " [0.40881687]\n",
      " [0.07780708]\n",
      " [0.33214018]\n",
      " [0.15862848]\n",
      " [0.07272042]\n",
      " [0.23474002]\n",
      " [0.19103241]\n",
      " [0.10889224]\n",
      " [0.43142426]\n",
      " [0.16880181]\n",
      " [0.45082894]\n",
      " [0.09984928]\n",
      " [0.25904295]\n",
      " [0.25075358]\n",
      " [0.43556896]\n",
      " [0.257159  ]\n",
      " [0.2503768 ]\n",
      " [0.26262245]\n",
      " [0.21759608]\n",
      " [0.09984928]\n",
      " [0.2799548 ]\n",
      " [0.337792  ]\n",
      " [0.13771665]\n",
      " [0.16314997]\n",
      " [0.16465712]\n",
      " [0.4046722 ]\n",
      " [0.26111528]\n",
      " [0.09532781]\n",
      " [0.36341372]\n",
      " [0.11963075]\n",
      " [0.10550113]\n",
      " [0.13074604]\n",
      " [0.30256218]\n",
      " [0.206104  ]\n",
      " [0.09984928]\n",
      " [0.14638282]\n",
      " [0.09984928]\n",
      " [0.21985681]\n",
      " [0.22852299]\n",
      " [0.30388093]\n",
      " [0.2605501 ]\n",
      " [0.316315  ]\n",
      " [0.4773926 ]\n",
      " [0.36209494]\n",
      " [0.2613037 ]\n",
      " [0.14958553]\n",
      " [0.39826676]\n",
      " [0.1951771 ]\n",
      " [0.25904295]\n",
      " [0.12302186]\n",
      " [0.27656367]\n",
      " [0.37886208]\n",
      " [0.25470987]\n",
      " [0.24717408]\n",
      " [0.15259986]\n",
      " [0.2756217 ]\n",
      " [0.16767144]\n",
      " [0.28843257]\n",
      " [0.52863604]\n",
      " [0.25621703]\n",
      " [0.13620949]\n",
      " [0.36850038]\n",
      " [0.34834212]\n",
      " [0.5440844 ]\n",
      " [0.3123587 ]\n",
      " [0.12528259]\n",
      " [0.19385833]\n",
      " [0.25131875]\n",
      " [0.38470232]\n",
      " [0.20836473]\n",
      " [0.22739261]\n",
      " [0.20836473]\n",
      " [0.21853806]\n",
      " [0.35606632]\n",
      " [0.39902034]\n",
      " [0.26996985]\n",
      " [0.18651092]\n",
      " [0.21175584]\n",
      " [0.2422758 ]\n",
      " [0.10908064]\n",
      " [0.3276187 ]\n",
      " [0.34325546]\n",
      " [0.09984928]\n",
      " [0.14657122]\n",
      " [0.3596458 ]\n",
      " [0.1872645 ]\n",
      " [0.11944235]\n",
      " [0.286737  ]\n",
      " [0.34419745]\n",
      " [0.2388847 ]\n",
      " [0.18236624]\n",
      " [0.07780708]\n",
      " [0.13300678]\n",
      " [0.25565183]\n",
      " [0.09984928]\n",
      " [0.07874906]\n",
      " [0.27467972]\n",
      " [0.2166541 ]\n",
      " [0.5951394 ]\n",
      " [0.24830444]\n",
      " [0.17746797]\n",
      " [0.1616428 ]\n",
      " [0.32554635]\n",
      " [0.09287868]\n",
      " [0.18274303]\n",
      " [0.1948003 ]\n",
      " [0.23944989]\n",
      " [0.31763375]\n",
      " [0.24302939]\n",
      " [0.09626978]\n",
      " [0.11944235]\n",
      " [0.14280331]\n",
      " [0.36341372]\n",
      " [0.21062547]\n",
      " [0.13281839]\n",
      " [0.14751318]\n",
      " [0.35079125]\n",
      " [0.24397136]\n",
      " [0.12170309]\n",
      " [0.36944234]\n",
      " [0.35173324]\n",
      " [0.32856065]\n",
      " [0.08779201]\n",
      " [0.23436323]\n",
      " [0.301055  ]\n",
      " [0.22098719]\n",
      " [0.1066315 ]\n",
      " [0.26073852]\n",
      " [0.26676714]\n",
      " [0.38884702]\n",
      " [0.30783722]\n",
      " [0.22645064]\n",
      " [0.09984928]\n",
      " [0.5452148 ]\n",
      " [0.3419367 ]\n",
      " [0.4344386 ]\n",
      " [0.33232856]\n",
      " [0.09984928]\n",
      " [0.16239639]\n",
      " [0.3197061 ]\n",
      " [0.0883572 ]\n",
      " [0.33214018]\n",
      " [0.28918612]\n",
      " [0.29238886]\n",
      " [0.02750565]\n",
      " [0.18123586]\n",
      " [0.09080633]\n",
      " [0.4118312 ]\n",
      " [0.11981914]\n",
      " [0.32064807]]\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks.utils import *\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, train_dataset = get_housing_data()\n",
    "\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape, '\\n')\n",
    "\n",
    "print('The original dataset looks as follows:')\n",
    "train_dataset.df.head()\n",
    "\n",
    "# print(train_dataset[0])\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is now ready and can be used to train our classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a Classifier Model\n",
    "\n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ be our data with $N$ samples and $D$ feature dimensions. With our classifier model, we want to predict binary labels $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{N\\times 1}$. Our classifier model should be of the form\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}  = \\sigma \\left( \\mathbf{X} \\cdot \\mathbf{w} \\right), $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight matrix of our model.\n",
    "\n",
    "The **sigmoid function** $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n",
    "\n",
    "$$ \\sigma(t) = \\frac{1}{1+e^{-t}}, $$\n",
    "\n",
    "is used to squash the outputs of the linear layer into the interval $[0, 1]$. Remember that the sigmoid function is a real-valued function. When applying it on a vector, the sigmoid is operating componentwise.\n",
    "\n",
    "The output of the sigmoid function can be seen as the probability that our sample is indicating a house that can be categorized as ```expensive```. As the probability gets closer to 1, our model is more confident that the input sample is in the class ```expensive```.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Take a look at the implementation of the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. To create a <code>Classifier</code> object, you need to define the number of features that our classifier model takes as input.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss: Binary Cross Entropy\n",
    "\n",
    "For a binary classification like our task, we use a loss function called Binary Cross-Entropy (BCE).\n",
    "\n",
    "$$BCE(y,\\hat{y}) =- y \\cdot log(\\hat y ) - (1- y) \\cdot log(1-\\hat y) $$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the ground truth and $\\hat y\\in\\mathbb{R}$ is the predicted probability of the house being expensive.\n",
    "\n",
    "Since the BCE function is a non-convex function, there is no closed-form solution for the optimal weights vector. In order to find the optimal parameters for our model, we need to use numeric methods such as Gradient Descent. But let us have a look at that later. First, you have to complete your first task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>In <code>exercise_code/networks/loss.py</code> complete the implementation of the BCE loss function. You need to write the forward and backward pass of BCE as <code>forward()</code> and <code>backward()</code> function. The backward pass of the loss is needed to later optimize your weights of the model. You can test your implementation by the included testing code in the cell below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan, -inf, -inf, ...,  nan, -inf,  nan],\n",
       "       [-inf,  nan,  nan, ..., -inf,  nan, -inf],\n",
       "       [-inf,  nan,  nan, ..., -inf,  nan, -inf],\n",
       "       ...,\n",
       "       [ nan, -inf, -inf, ...,  nan, -inf,  nan],\n",
       "       [-inf,  nan,  nan, ..., -inf,  nan, -inf],\n",
       "       [ nan, -inf, -inf, ...,  nan, -inf,  nan]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()\n",
    "y_truth = y_train\n",
    "y_out = y_train\n",
    "\n",
    "bce_loss.forward(y_out, y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE forward incorrect. Expected: [-1.33792078  5.768321  ] Evaluated: -4.4304002189580975\n",
      "BCEBackwardTest failed due to exception: unsupported operand type(s) for -: 'float' and 'NoneType'.\n",
      "Test cases are still failing! Tests passed: 0/2\n",
      "(2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of exercise_code.networks.classifier failed: Traceback (most recent call last):\n",
      "  File \"/Users/stevenmoore/.conda/envs/I2DL/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/stevenmoore/.conda/envs/I2DL/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/stevenmoore/.conda/envs/I2DL/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/Users/stevenmoore/.conda/envs/I2DL/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/stevenmoore/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/networks/classifier.py\", line 63\n",
      "    return y\n",
      "         ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()\n",
    "print (BCETest(bce_loss)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Backpropagation\n",
    "\n",
    "The backpropagation algorithm allows the information from the loss flowing backward through the network in order to compute the gradient of the loss function $L$ w.r.t the weights $w$ of the model. \n",
    "\n",
    "The key idea of backpropagation is decomposing the derivatives by applying the chain rule to the loss function.\n",
    "\n",
    "$$ \\frac{\\partial L(w)}{\\partial w} = \\frac{\\partial L(w)}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial w}$$\n",
    "\n",
    "You have already completed the `forward()` and `backward()` pass of the loss function, which can be used to compute the derivative  $\\frac{\\partial L(w)}{\\partial \\hat y}$. In order to compute the second term $\\frac{\\partial \\hat y}{\\partial w}$, we need to implement a similar `forward()` and `backward()` method in our `Classifier` class.\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "The backward pass consists of computing the derivative $\\frac{\\partial \\hat y}{\\partial w}$. Again, we can decompose this derivative by the chain rule: For $s = X \\cdot w$ we obtain\n",
    "\n",
    "$$\\frac{\\partial \\hat y}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial s} \\cdot \\frac{\\partial s}{\\partial w}$$\n",
    "\n",
    "\n",
    "**Hint:** Taking track of the dimensions in higher-dimensional settings can make the task a little bit complicated. Make sure you understand the operations here. If you have difficulties, first try to understand the forward and backward pass if the input is only one sample consisting of $D+1$ features. Then our data matrix has dimension $X \\in \\mathbb{R}^{1 \\times (D+1)}$. After you understood this situation, you can go back to the setting where our data matrix has dimension $X \\in \\mathbb{R}^{N \\times (D+1)}$ and consists of $N$ samples each having $D+1$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>forward()</code> and <code>backward()</code> pass as well as the <code>sigmoid()</code> function in the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. Check your implementation using the following testing code.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f148c6d7f01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_test\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/classifier_test.py\u001b[0m in \u001b[0;36mtest_classifier\u001b[0;34m(Classifier)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m\"\"\"Test the Classifier\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_results_to_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/base_tests.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/classifier_test.py\u001b[0m in \u001b[0;36mdefine_tests\u001b[0;34m(self, Classifier)\u001b[0m\n\u001b[1;32m    155\u001b[0m         return [SigmoidMethodTest(Classifier),\n\u001b[1;32m    156\u001b[0m                 \u001b[0mForwardMethodTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mBackwardMethodTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 ]\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/base_tests.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_method_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/base_tests.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/classifier_test.py\u001b[0m in \u001b[0;36mdefine_tests\u001b[0;34m(self, Classifier)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBackwardMethodTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMethodTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdefine_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         return [ClassifierBackwardTest(Classifier)\n\u001b[0m\u001b[1;32m    147\u001b[0m                 ]\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WS20:21/I2DL/Programming /i2dl/exercise_04/exercise_code/tests/classifier_test.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Classifier)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         self.truth = np.concatenate(\n\u001b[0;32m--> 113\u001b[0;31m             (sample_x, np.ones((2, 1))), axis=1) * sample_y * (1 - sample_y)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.classifier_test import *\n",
    "test_classifier(Classifier(num_features=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer and Gradient Descent\n",
    "\n",
    "Previously, we have successfully dealt with the loss function, which is a method of measuring how well our model fits the given data. The idea of the training process is to adjust iteratively the weights of our model in order to minimize the loss function. \n",
    "\n",
    "And this is where the optimizer comes in. In each training step, the optimizer updates the weights of the model w.r.t. the output of the loss function, thereby linking the loss function and model parameters together. The goal is to obtain a model which is accurately predicting the class for a new sample.\n",
    "\n",
    "\n",
    "Any discussion about optimizers needs to begin with the most popular one, and it’s called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It’s fast, robust, and flexible. Here’s how it works:\n",
    "\n",
    "\n",
    "0. Initialize the weights with random values.\n",
    "1. Calculate loss with the current weights and the loss function.\n",
    "2. Calculate the gradient of the loss function w.r.t. the weights.\n",
    "3. Update weights with the corresponding gradient.\n",
    "4. Iteratively perform Step 1 to 3 until converges.\n",
    "\n",
    "The name of the optimizer already hints at the required concept: We use gradients which are very useful for minimizing a function. The gradient of the loss function w.r.t to the weights $w$ of our model tells us how to change our weights $w$ in order to minimize our loss function. \n",
    "\n",
    "The weights are updated each step as follows:\n",
    "$$ w^{(n+1)} = w^{(n)} - \\alpha \\cdot \\frac {dL}{dw}, $$\n",
    "where $ \\frac {dL}{dw}$ is the gradient of your loss function w.r.t. the weights $w$ and $\\alpha$ is the learning rate which is a predefined positive scalar determining the size of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>In our model, we will use gradient descent to update the weights. Take a look at the <code>Optimizer</code> class in the file <code>networks/optimizer.py</code>. Your task is now to implement the gradient descent step in the <code>step()</code> method. You can test your implementation by the following testing code.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import Optimizer\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.optimizer_test import *\n",
    "TestClassifier=Classifier(num_features=2)\n",
    "TestClassifier.initialize_weights()\n",
    "test_optimizer(Optimizer(TestClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We have now implemented all the necessary parts of our training process, namely:\n",
    "- **Classifier Model:** We set up a simple classifier model and you implemented the corresponding ```forward()``` and ```backward()``` methods.\n",
    "- **Loss function:** We chose the Binary Cross Entropy Loss for our model to measure the distance between the prediction of our model and the ground-truth labels. You implemented a forward and backward pass for the loss function.\n",
    "- **Optimizer**: We use the Gradient Descent method to update the weights of our model. Here, you implemented the ```step()``` function which performs the update of the weights. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Before we start our training and put all the parts together, let us shortly talk about the weight initialization. In <code>networks/classifier.py</code> you can check the <code>Classifier</code> class. It contains a method called <code>initialize_weights()</code> that randomly initializes the weights of our classifier model. Later in the lecture, we will learn about more efficient methods to initialize the weights. But for now, a random initialization as it happens in the <code>initialize_weights()</code> method is sufficient.</p>\n",
    "</div>\n",
    "\n",
    "Let's start with our classifier model and look at its performance before any training happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "#initialization\n",
    "model = Classifier(num_features=1)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_out, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the predictions of our model without any training are very bad. Let's see how the performance improves when we start our training, which means that we update our weights by applying the gradient descent method. The following cell combines the forward- and backward passes with the gradient update step and performs a training step for our classifier:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Note that the <code>Classifier</code> class is derived from the more general <code>Network</code> class. It is worth having a look at the basis class <code>Network</code> in the file <code>exercise_code/networks/base_networks.py</code>. We will make use of the <code>__call__()</code> method, which computes the forward and backward pass of your classifier. In a similar manner, we use the <code>__call__()</code> function for our Loss function.</p>\n",
    "</div>\n",
    "\n",
    "The following cell performs training with 400 training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import *\n",
    "from exercise_code.networks.classifier import *\n",
    "# Hyperparameter Setting, we will specify the loss function we use, and implement the optimizer we finished in the last step.\n",
    "num_features = 1\n",
    "\n",
    "# initialization\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "loss_func = BCE() \n",
    "learning_rate = 5e-1\n",
    "loss_history = []\n",
    "opt = Optimizer(model,learning_rate)\n",
    "\n",
    "steps = 400\n",
    "# Full batch Gradient Descent\n",
    "for i in range(steps):\n",
    "    \n",
    "    # Enable your model to store the gradient.\n",
    "    model.train()\n",
    "    \n",
    "    # Compute the output and gradients w.r.t weights of your model for the input dataset.\n",
    "    model_forward, model_backward = model(X_train)\n",
    "    \n",
    "    # Compute the loss and gradients w.r.t output of the model.\n",
    "    loss, loss_grad = loss_func(model_forward, y_train)\n",
    "    \n",
    "    # Use back prop method to get the gradients of loss w.r.t the weights.\n",
    "    grad = loss_grad * model_backward\n",
    "    \n",
    "    # Compute the average gradient over your batch\n",
    "    grad = np.mean(grad, 0, keepdims = True)\n",
    "\n",
    "    # After obtaining the gradients of loss with respect to the weights, we can use optimizer to\n",
    "    # do gradient descent step.\n",
    "    opt.step(grad.T)\n",
    "    \n",
    "    # Average over the loss of the entire dataset and store it.\n",
    "    average_loss = np.mean(loss)\n",
    "    loss_history.append(average_loss)\n",
    "    if i%10 == 0:\n",
    "        print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our average loss is decreasing as expected. Let us visualize the average loss and the prediction after our short training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss history to see how it goes after several steps of gradient descent.\n",
    "plt.plot(loss_history, label = 'Train Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "y_out, _ = model(X_train)\n",
    "\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train, label = 'Ground Truth')\n",
    "inds = X_train.argsort(0).flatten()\n",
    "plt.plot(X_train[inds], y_out[inds], color='r', label = 'Prediction')\n",
    "plt.title('Prediction of our trained model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good already and our model gets better in explaining the underlying relationship of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solver\n",
    "\n",
    "Now we want to put everything we have learned so far together in an organized and concise way, that provides easy access to train a network/model in your own script/code. The purpose of a solver is mainly to provide an abstraction for all the gritty details behind training your parameters, such as logging your progress, optimizing your model, and handling your data.\n",
    "\n",
    "This part of the exercise will require you to complete the missing code in the ```Solver``` class and to train your model end to end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open the file <code>exercise_code/solver.py</code> and have a look at the <code>Solver</code> class. The <code>_step()</code> function is representing one single training step. So when using the Gradient Descent method, it represents one single update step using the Gradient Descent method. Your task is now to finalize this <code>_step()</code> function. You can test your implementation with the testing code included in the following cell.</p>\n",
    "    <p> <b>Hint</b>: The implementation of the <code>_step()</code> function is very similar to the implementation of a training step as we observed above. You may have a look at that part first. </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.solver_tests import *\n",
    "weights = np.array([[0.1],[0.1]])\n",
    "TestClassifier = Classifier(num_features=1)\n",
    "TestClassifier.initialize_weights(weights)\n",
    "learning_rate = 5e-1\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "loss = BCE()\n",
    "solver = Solver(TestClassifier,data,loss,learning_rate,verbose=True)\n",
    "\n",
    "test_solver(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having successfully implemented the `step()` function in the `Optimizer` class, let us now train our classifier. We train our model with a learning rate $ \\lambda = 0.1$ and with 25000 epochs. Your model should reach an accuracy which is higher than 85%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.utils import test_accuracy\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "# Select the number of features, you want your task to train on.\n",
    "# Feel free to play with the sizes.\n",
    "num_features = 1\n",
    "\n",
    "# initialize model and weights\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "\n",
    "if np.shape(X_val)[1]==1:\n",
    "    plt.scatter(X_val, y_val, label = \"Ground Truth\")\n",
    "    inds = X_test.flatten().argsort(0)\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "#We use the BCE loss\n",
    "loss = BCE()\n",
    "\n",
    "# Please use these hyperparmeter as we also use them later in the evaluation\n",
    "learning_rate = 1e-1\n",
    "epochs = 25000\n",
    "\n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training.\n",
    "solver = Solver(model, \n",
    "                data, \n",
    "                loss,\n",
    "                learning_rate, \n",
    "                verbose=True, \n",
    "                print_every = 1000)\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)\n",
    "\n",
    "\n",
    "# Test final performance\n",
    "y_out, _ = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "During the training process losses in each epoch are stored in the lists `solver.train_loss_history` and `solver.val_loss_history`. We can use them to plot the training result easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.title('Prediction of your trained model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Save your BCE Loss, Classifier and Solver for Submission\n",
    "\n",
    "Your model should be trained now and able to predict whether a house is expensive or not. Hooooooray, you trained your very first model! The model will be saved as a pickle file to `models/simple_classifier.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"BCE_class\": BCE,\n",
    "        \"Classifier_class\": Classifier,\n",
    "        \"Optimizer\": Optimizer,\n",
    "        \"Solver_class\": Solver\n",
    "    },\n",
    "    file_name=\"simple_classifier.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the neccessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Execute the cell below to create a zipped folder for upload.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"logistic_regression.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Submission Goals\n",
    "\n",
    "For this exercise we only test your implementations which are tested throughout the notebook.  In total we have 10 test cases where you are required to complete 8 of. Here is an overview split among the notebook:\n",
    "\n",
    "- Goal: \n",
    "    - To implement: \n",
    "        1. `exercise_code/networks/loss.py`: `forward()`, `backward()`\n",
    "        2. `exercise_code/networks/classifier.py`: `forward()`, `backward()`, `sigmoid()`\n",
    "        3. `exercise_code/networks/optimizer.py`: `step()`\n",
    "        4. `exercise_code/solver.py`: `_step()`\n",
    "\n",
    "    - Test cases:\n",
    "      1. Does `forward()` of `BCE` return the correct value?\n",
    "      2. Does `backward()` of `BCE` return the correct value?\n",
    "      3. Does `sigmoid()` of `Classifier` return the correct value when `x=0`?\n",
    "      4. Does `sigmoid()` of `Classifier` return the correct value when `x=np.array([0,0,0,0,0])`?\n",
    "      5. Does `sigmoid()` of `Classifier` return the correct value when `x=100`?\n",
    "      6. Does `sigmoid()` of `Classifier` return the correct value when `x=np.asarray([100, 100, 100, 100, 100])`?\n",
    "      7. Does `forward()` of `Classifier` return the correct value?\n",
    "      8. Does `backward()` of `Classifier` return the correct value?\n",
    "      9. Does `Optimizer` update the model parameter correctly?\n",
    "      10. Does `Solver` update the model parameter correctly?\n",
    "\n",
    "\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n",
    "- Threshold to clear exercise: 80\n",
    "- Submission start: __Nov 26, 2020 13.00__\n",
    "- Submission deadline: __Dec 02, 2020 15.59__. \n",
    "- You can make multiple submission until the deadline. Your __best submission__ will be considered for bonus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
