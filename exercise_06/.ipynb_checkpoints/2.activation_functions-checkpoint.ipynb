{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Actication Functions\n",
    "\n",
    "In this notebook, we want to take a look at four common activation functions: The sigmoid function, the ReLu function, the Leaky ReLu function and the Tanh function. After this notebook, we will have completed the implementation for an activation layer for each of these activation functions. This includes the implementation of the `forward()` and of the `backward()` functions.\n",
    "\n",
    "__Note__: We provide you with the implementation of the Sigmoid and the ReLU activation function for your convenience. Rememberthat in Exercise4, you already worked on the Sigmoid class.\n",
    "\n",
    "Let us start by loading the necessary file `exercise_code/networks/layer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from exercise_code.tests.layer_tests import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sigmoid Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid non-linearity has the mathematical form $f(x) = \\frac{1}{1 + e^{-x}}$ and is shown in the image below. As alluded to in the previous section, it takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. \n",
    "\n",
    "The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely used. \n",
    "\n",
    "<img src=https://pytorch.org/docs/stable/_images/Sigmoid.png alt=\"Figure4\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check <code>Sigmoid</code> Class </h3>\n",
    "    <p>As mentioned above, we will provide you with the implementation of the Sigmoid layer. Take a look at the <code>Sigmoid</code> class in <code>exercise_code/networks/layer.py</code>. We have implmented the forward and backward pass of this activation layer. Try to understand why we use <b>elementwise product</b> instead of dot product here to compute the gradient $dx$. That will be helpful for your later implementation of other activation functions.\n",
    "        <p> <b>Note:</b> Make sure to understand the use of the <code>cache</code> variable. We store the output of the forward pass <code>outputs</code> to make use of it in the backward pass.</p>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ReLU Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x) = max(0, x)$. In other words, the activation is simply thresholded at zero (see image below).\n",
    "\n",
    "<img src=https://pytorch.org/docs/stable/_images/ReLU.png alt=\"Figure2\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Implement the <code>ReLu</code> class </h3>\n",
    "    <p>As mentioned above, we will provide you with the implementation of the ReLu layer. Take a look at the <code>ReLu</code> class in <code>exercise_code/networks/layer.py</code>. We have implmented the forward and backward pass of this activation layer.\n",
    "        <p> <b>Note:</b> Make sure to understand the use of the <code>cache</code> variable. Here, we don't store the output of the forward pass but rather the input <code>x</code> of the forward pass, which we will need in the calculations of the backward pass. The cache variable is thus used differently in the implementations of the activation layers. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Leaky ReLU Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when $x<0$, a leaky ReLU has a small negative slope (for example, 0.01). That is, the function computes $f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x>=0) (x)$ where $\\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear. \n",
    "\n",
    "<img src=https://pytorch.org/docs/stable/_images/LeakyReLU.png alt=\"Figure3\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement the <code>LeakyReLu</code> class </h3>\n",
    "    <p> Now, it is your turn to implement your own <code>LeakyRelu</code> class in <code>exercise_code/networks/layer.py</code> by completing the <code>forward</code> and the <code>backward</code> functions. You can test your implementation in the following cell. </p>\n",
    "    <p> <b>Note:</b> Always remember to return a cache in <code>forward</code> for later backpropagation in <code>backward</code>. As we have seen above,the <code>cache</code> variable can be used differently.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LeakyReluTest()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tanh Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tanh non-linearity is shown on the image below. It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\\tanh(x) = 2 \\cdot \\sigma(2x) -1$.\n",
    "\n",
    "<img src=https://pytorch.org/docs/stable/_images/Tanh.png alt=\"Figure3\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement the <code>Tanh</code> class </h3>\n",
    "    <p> Now, it is your turn to implement your own <code>Tanh</code> class in <code> exercise_code/networks/layer.py</code> by completing the <code>forward</code> and the <code>backward</code> functions similar to what we have done in the <code>Sigmoid</code> class. You can test your implementation in the following cell. </p>\n",
    "    <p> <b>Note:</b> Always remember to return a cache in <code>forward</code> for later back propagation in <code>backward</code>. As we have seen above, the <code>cache</code> variable can be used differently.</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TanhTest()())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
