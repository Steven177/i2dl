{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edh6cUq2gM5Q"
   },
   "source": [
    "#Sentiment Analysis using RNNs\n",
    "\n",
    "This notebook addresses sentiment analysis task on text data. We use [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/). In this dataset, the task is to classify the sentiment IMDB movie review comments as positive or negative. There are 25k labeled training and 25k labeled test examples in the initial data. For feasibility as an I2DL exercise, very long reviews (e.g. longer 150 words) should be ignored.\n",
    "\n",
    "This notebook uses Pytorch's own LSTM and Embedding implementations. For the exercise, students should be asked to implement these layers themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtbPxbduiT9e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47zLtoJRI9Uh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eBPfjZ8jiZT"
   },
   "source": [
    "## Data Download\n",
    "The data is pulicly available, and this notebook uses the provided train and test splits. For the exercise we should create our own splits. Otherwise one can overfit to the test set whose labels are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K67vhuirJlEY"
   },
   "outputs": [],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYcqpLaMpv2C"
   },
   "source": [
    "## Data Preparation\n",
    "Preprocessing text data is tricky. There are two important steps we have to take care of:\n",
    "\n",
    "1. As we use a word-based language model, we need to <b>tokenize</b> each text in lists of words. Tokenization is relatively easy in English, but can be tricky in some other languages.\n",
    "\n",
    "2. We need to assign an integer index as an id to each word. The mapping that contains word -> id is often called <b>vocabulary</b>.\n",
    "\n",
    "The below helper functions loads and tokenizes data while creating the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGeIMYnlKRqp"
   },
   "outputs": [],
   "source": [
    "def load_train_data(max_vocab_size=5000, max_len=250):\n",
    "    base_dir = './aclImdb/train/'\n",
    "    data = []  # list of tuples of (raw_text, word_list, label (0 or 1))\n",
    "\n",
    "    # Infrequent words can noise the training.\n",
    "    # Therefore we select only the max_vocab_size most commong words.\n",
    "\n",
    "    # 1. Collect and tokenize all sentences and keep track of the word \n",
    "    # frequencies on the fly. Counter is a useful data structure to store\n",
    "    # frequencies.\n",
    "    word_freqs = Counter()\n",
    "    for label in ('pos', 'neg'):\n",
    "        dir = base_dir + label\n",
    "        for text_file in os.listdir(dir):\n",
    "            with open(os.path.join(dir, text_file)) as f:\n",
    "                text = f.read().strip()\n",
    "                # Below tokenization will get words w/o special characters\n",
    "                # in lower case\n",
    "                words = [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n",
    "            if len(words) > max_len:  # Ignore very long sequences\n",
    "                continue\n",
    "            word_freqs.update(words)\n",
    "            data.append((text, words, 1 if label == 'pos' else 0))\n",
    "\n",
    "    # 2. Create the \"vocabulary\", a dictionary word -> integer id.\n",
    "    # Note we have two special words:\n",
    "    ## <eos>: end of sequence, used for padding\n",
    "    ## <unk>: unknown, used for infrequent words\n",
    "    word2id = {'<eos>': 0, '<unk>': 1}\n",
    "    for word, freq in word_freqs.most_common(\n",
    "        min(max_vocab_size, len(word_freqs))\n",
    "    ):\n",
    "        word2id[word] = len(word2id)\n",
    "    \n",
    "    # 3. Store the inverse vocabulary (id -> word). For this task, id2word\n",
    "    # is not necessary, but it could be used e.g. in text generation.\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    assert len(word2id) == len(id2word)\n",
    "\n",
    "    # 4. Replace words with their integer ids\n",
    "    for raw_text, words, label in data:\n",
    "        for i in range(len(words)):\n",
    "            words[i] = word2id.get(words[i], word2id['<unk>'])\n",
    "    # Return vocabulary, inverse vocabulary and data. Same vocabulary must\n",
    "    # be used for the test data!\n",
    "    return word2id, id2word, data\n",
    "\n",
    "\n",
    "def load_test_data(word2id, max_len=250):\n",
    "    # Same logic as the train data but with a given vocabulary.\n",
    "    data = []\n",
    "    base_dir = './aclImdb/test/'\n",
    "    \n",
    "    for label in ('pos', 'neg'):\n",
    "        dir = base_dir + label\n",
    "        for text_file in os.listdir(dir):\n",
    "            with open(os.path.join(dir, text_file)) as f:\n",
    "                text = f.read().strip()\n",
    "            words = [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n",
    "            if len(words) > max_len:  # Ignore very long sequences\n",
    "                continue\n",
    "            words = [word2id.get(word, word2id['<unk>']) for word in words]\n",
    "            data.append((text, words, 1 if label == 'pos' else 0))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEMnQJEOsYZK"
   },
   "source": [
    "We can now wrap the above function in a Pytorch Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2_4xT_ZTMjA"
   },
   "outputs": [],
   "source": [
    "class IMDB(Dataset):\n",
    "    def __init__(self, is_train=True, w2i=None, max_len=150):\n",
    "\n",
    "        # NOTE: Larger max_len will increase # data, therefore test accuracy \n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        if is_train:\n",
    "            assert not w2i, 'Provided vocabulary is not supported in training'\n",
    "            w2i, _, data = load_train_data(max_len=max_len)\n",
    "            self.w2i = w2i\n",
    "            # Ignore i2w for now\n",
    "        else:\n",
    "            assert w2i, 'You must provide the training w2i for the test data!'\n",
    "            data = load_test_data(w2i, max_len=max_len)\n",
    "\n",
    "        self.w2i = w2i\n",
    "        self.data = data\n",
    "\n",
    "        # Even if we support batching sequences with different lengths, \n",
    "        # sorting the data based on size is important! Otherwise, there will be \n",
    "        # a lot of padding that will cause wasting computational resources.\n",
    "        # \n",
    "        # The \"reverse=True\" argument in .sort makes longest sequences \n",
    "        # come first during training. Therefore, we get out-of-memory errors \n",
    "        # at the beginning if batch size is too large :)!\n",
    "        self.data.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _, words, label = self.data[index]\n",
    "        return {\n",
    "            'data': torch.tensor(words).long(),\n",
    "            'label': torch.tensor(label).float()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idk7f3GNwqqp"
   },
   "source": [
    "Now we can load the data. Below cell loads all dataset into the RAM, so it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7ly4oqDwqN9",
    "outputId": "9a41ad86-4dfd-4cbf-d3e9-d8d0e9293fa0"
   },
   "outputs": [],
   "source": [
    "# Having larger max_len increase the training data size, therefore\n",
    "# the test acuracy. Picking a small max_len is important for those\n",
    "# who use CPU.\n",
    "max_len = 150\n",
    "\n",
    "# This will create the vocab (w2i) and store it as a member variable\n",
    "train_dataset = IMDB(is_train=True, max_len=max_len)\n",
    "\n",
    "# When the goal is submitting formal results, there shouldn't be a\n",
    "# max_len on test/val data even if there is one on training data.\n",
    "# Here we make this exception because o/w things become too slow.\n",
    "test_dataset = IMDB(is_train=False, w2i=train_dataset.w2i, max_len=max_len)\n",
    "\n",
    "# Compare the output with original 25k in both splits\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgzgtKJOyWm4"
   },
   "source": [
    "## Minibatching\n",
    "Now we come to the hardest part of dealing with text data :D! Unlike in images, sequences may have different lengths. Therefore, it is necessary to pad sentences to make them the same size. \n",
    "\n",
    "To be able to use PyTorch's standard DataLoader class, we have to define a custom collate function (see [here](https://pytorch.org/docs/stable/data.html) for its use). We make use of [pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) function of PyTorch. By default sequences are padded with 0 (\\<eos>) words.\n",
    "\n",
    "Apart from data and labels, note that we also keep track of sequence lengths. The lengths will be used by the model to ignore padded elements during back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjZa7fiSTjOL"
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch]).view(-1, 1)\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "119blNLR2Wjo"
   },
   "source": [
    "## Model Creation\n",
    "\n",
    "### Model Components\n",
    "Our text classifier model consists of three stages:\n",
    "\n",
    "1. <b>Embedding layer</b> converts integer indices to dense vectors. It is nothing but a randomly initialized matrix. Each row (or column depending on the implementation) of it corresponds to a word. Embedding can also be initialized with transfer learning (using Word2Vec, Glove e.t.c.), but we don't do it here.\n",
    "\n",
    "2. <b>RNN</b> processes the embeddings. In this implementation, the last hidden state of the RNN is used for classification. Alternatively, hidden state history could be used via some sort of attention.\n",
    "\n",
    "3. <b>Output layer</b> is an MLP (could be a Linear model) that produces probabilities from the last hidden state.\n",
    "\n",
    "### Minibatching\n",
    "The pack_padded_sequence function is used to process padded batches in PyTorch. However, it is a complex mechanism related to the cuDNN backend. When students are implementing the padding stuff, they should be asked to use the padded representation and select the last hidden state using a loop at the end.\n",
    "\n",
    "### Possible Enhancements\n",
    "Bidirectional RNNs, Dropouts, and multiple layers are possible model enhancements. However, except for dropout of embedding outputs and MLP input/hidden layers, enhancements can be too hard for an exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V7UVupJl9K-"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class TextClassifier(nn.Module):\n",
    "    # TODO: num_layers, bidirectional, dropout?\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence, lengths=None):\n",
    "        embeddings = self.embedding(sequence)\n",
    "        if lengths is not None:\n",
    "            embeddings = pack_padded_sequence(embeddings, lengths)\n",
    "     \n",
    "        last_state = self.rnn(embeddings)[-1]\n",
    "        if isinstance(last_state, tuple):  # includes cell state\n",
    "            last_state = last_state[0]\n",
    "\n",
    "        # NOTE: Below code should change for bidirectional + multi-layer\n",
    "        last_state = last_state.squeeze(0)  # N x D\n",
    "\n",
    "        return self.output(last_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEydjDy_GeJs"
   },
   "source": [
    "## Training\n",
    "It is just a regular PyTorch training loop. Note the gradient clipping to avoid exploding gradients problem and the collate_fn argument used for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyOieqJbkhuL",
    "outputId": "00271c57-b1e8-49cc-9100-f7b160d1e9be"
   },
   "outputs": [],
   "source": [
    "# Training configs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using {}...\\n'.format(device))\n",
    "\n",
    "epochs = 5\n",
    "model = TextClassifier(len(train_dataset.w2i), 64, 64).to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "gclip = 20\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, batch_size=4, collate_fn=collate, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "  test_dataset, batch_size=4, collate_fn=collate, drop_last=False\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}...'.format(e))\n",
    "    model.train()\n",
    "    num_corrects = 0\n",
    "    num_labels = 0\n",
    "    total_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        seq = data['data'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        seq_lens = data['lengths']\n",
    "\n",
    "        model.zero_grad()\n",
    "        pred = model(seq, seq_lens)\n",
    "        loss = F.binary_cross_entropy(pred, label, reduction='mean')\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=gclip)\n",
    "        optim.step()\n",
    "        \n",
    "        num_corrects += ((pred > 0.5) == label).sum().item()\n",
    "        num_labels += label.numel()\n",
    "        total_loss += loss.item() * label.numel()\n",
    "        if i % 500 == 0:\n",
    "            print('Iter: {}, Loss: {}, Accuracy: {}'.format(\n",
    "                i, total_loss / num_labels, num_corrects / num_labels\n",
    "            ))\n",
    "\n",
    "    print('Training loss/accuracy: {}/{}'.format(\n",
    "        total_loss / num_labels, num_corrects / num_labels\n",
    "    ))\n",
    "\n",
    "    print('\\nStarting evaluation...')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_corrects = 0\n",
    "        num_labels = 0\n",
    "        for i, data in enumerate(test_loader):\n",
    "            seq = data['data'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "            seq_lens = data['lengths']\n",
    "    \n",
    "            pred = model(seq, seq_lens) > 0.5\n",
    "\n",
    "            num_corrects += (pred == label).sum().item()\n",
    "            num_labels += label.numel()\n",
    "            if i % 500 == 0:\n",
    "                print('Iter: {}, Accuracy: {}'.format(\n",
    "                    i, num_corrects / num_labels\n",
    "                ))\n",
    "    print('Accuracy: {}'.format(num_corrects / num_labels))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCKZyl9flhYw",
    "outputId": "b574fa78-55b1-47c0-a4b8-a216ead16be3"
   },
   "outputs": [],
   "source": [
    "text = ''\n",
    "w2i = train_dataset.w2i\n",
    "while True:\n",
    "    text = input()\n",
    "    if text == 'exit':\n",
    "        break\n",
    "\n",
    "    words = torch.tensor([\n",
    "        w2i.get(word.lower(), w2i['<unk>'])\n",
    "        for word in re.split(r'\\W+', text)\n",
    "    ]).long().to(device).view(-1, 1)  # T x B\n",
    "\n",
    "    pred = model(words).item()\n",
    "    sent = pred > 0.5\n",
    "    \n",
    "    print('Sentiment -> {}, Confidence -> {}'.format(\n",
    "        ':)' if sent else ':(', pred if sent else 1 - pred\n",
    "    ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCQnveQF8fIp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
